#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§è…è´¥åˆ†æå™¨ v2.0
ä¸“é—¨é’ˆå¯¹å¤æ‚å…³ç³»ç½‘å’Œéšæ™¦è…è´¥è¡Œä¸ºçš„æ™ºèƒ½åˆ†æç³»ç»Ÿ
"""

import json
import re
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from typing import Dict, List, Any, Tuple, Set
import math

class AdvancedCorruptionAnalyzer:
    """é«˜çº§è…è´¥åˆ†æå™¨ - v2.0æ ¸å¿ƒå¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.semantic_patterns = self._load_semantic_patterns()
        self.behavioral_patterns = self._load_behavioral_patterns()
        self.network_analyzer = NetworkAnalyzer()
        self.temporal_analyzer = TemporalAnalyzer()
        self.context_analyzer = ContextAwareAnalyzer()
        
    def _load_semantic_patterns(self) -> Dict[str, List[str]]:
        """åŠ è½½è¯­ä¹‰æ¨¡å¼åº“"""
        return {
            "éšæ™¦èµ„é‡‘": [
                "è¡¨ç¤ºä¸€ä¸‹", "å¿ƒæ„", "ä¸€ç‚¹å°æ„æ€", "å¸®å¿™è´¹", 
                "è¾›è‹¦è´¹", "èŒ¶æ°´è´¹", "é‚£ç¬”æ¬¾", "ä¹‹å‰è¯´çš„æ•°",
                "çº¦å®šçš„æ•°", "é‚£ä¸ªæ•°", "å‡†å¤‡å¥½äº†", "å®‰æ’å¥½äº†",
                "æ”¾åœ¨ä½ è½¦ä¸Š", "çŸ¥é“æ€ä¹ˆå¤„ç†", "éƒ½æ¸…æ¥šäº†"
            ],
            "æƒåŠ›æ»¥ç”¨": [
                "æ‰“ä¸ªæ‹›å‘¼", "å…³ç…§ä¸€ä¸‹", "å¼€ç»¿ç¯", "é€šèé€šè",
                "ç‰¹æ®Šå¤„ç†", "èµ°å¿«é€Ÿé€šé“", "æŒ‰è€è§„çŸ©", "ç…§æ—§",
                "ä½ çŸ¥é“çš„", "éƒ½æ‡‚", "ä¸ç”¨å¤šè¯´", "æ˜ç™½çš„",
                "æŒ‰æƒ¯ä¾‹", "ç‰¹äº‹ç‰¹åŠ", "çµæ´»å¤„ç†"
            ],
            "è¯æ®å¤„ç†": [
                "æ¸…ç†ä¸€ä¸‹", "ä¸ç•™ç—•è¿¹", "è¯¥åˆ çš„åˆ ", "åªæœ‰æˆ‘ä»¬çŸ¥é“",
                "å¤©çŸ¥åœ°çŸ¥", "å£å¤´è¯´", "åˆ«ç•™è®°å½•", "å½“é¢èŠ",
                "ç”µè¯é‡Œè¯´", "åˆ«å‘å¾®ä¿¡", "æ’¤å›å§", "åˆ é™¤è®°å½•"
            ],
            "ç§˜å¯†ä¼šé¢": [
                "ç§ä¸‹èŠèŠ", "è€åœ°æ–¹", "æ–¹ä¾¿çš„æ—¶å€™", "æ‰¾ä¸ªæ—¶é—´",
                "å•ç‹¬èŠèŠ", "åªæœ‰æˆ‘ä»¬", "åˆ«å‘Šè¯‰åˆ«äºº", "ä¿å¯†",
                "æ™šä¸Šè§", "å‘¨æœ«æœ‰ç©ºå—", "ä¸‹ç­å", "éå·¥ä½œæ—¶é—´"
            ],
            "å‚æ•°å®šåˆ¶": [
                "æŠ€æœ¯å‚æ•°", "è§„æ ¼ä¹¦", "æŒ‰ä½ ä»¬è¦æ±‚", "ç¬¦åˆä½ ä»¬",
                "é‡èº«å®šåˆ¶", "è°ƒæ•´å‚æ•°", "ä¿®æ”¹è¦æ±‚", "æŠ€æœ¯è§„æ ¼",
                "ä½ ä»¬å…ˆè‰æ‹Ÿ", "æŒ‰è‰æ¡ˆèµ°", "ä½ ä»¬çš„ä¼˜åŠ¿"
            ]
        }
    
    def _load_behavioral_patterns(self) -> Dict[str, Any]:
        """åŠ è½½è¡Œä¸ºæ¨¡å¼åº“"""
        return {
            "æ—¶é—´å¼‚å¸¸": {
                "æ·±å¤œæ—¶æ®µ": (22, 6),  # 22:00-6:00
                "å‘¨æœ«æ—¶æ®µ": [5, 6],   # å‘¨å…­ã€å‘¨æ—¥
                "å·¥ä½œæ—¶é—´": (9, 18)   # 9:00-18:00
            },
            "é¢‘ç‡å¼‚å¸¸": {
                "çªç„¶å¢åŠ é˜ˆå€¼": 3.0,  # çªç„¶å¢åŠ 3å€
                "çªç„¶æ²‰é»˜é˜ˆå€¼": 0.2,  # çªç„¶å‡å°‘åˆ°20%
                "é«˜é¢‘ä¼šé¢é˜ˆå€¼": 5     # æ¯å‘¨5æ¬¡ä»¥ä¸Š
            },
            "å…³ç³»å¼‚å¸¸": {
                "è·¨çº§æ²Ÿé€š": True,
                "éå·¥ä½œå…³ç³»": True,
                "ç§äººä¼šé¢": True
            }
        }
    
    def analyze(self, chat_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„è…è´¥åˆ†æ
        
        Args:
            chat_data: èŠå¤©è®°å½•åˆ—è¡¨
            
        Returns:
            å®Œæ•´çš„åˆ†ææŠ¥å‘Š
        """
        print("ğŸ” å¼€å§‹é«˜çº§è…è´¥åˆ†æ...")
        
        # 1. è¯­ä¹‰åˆ†æ
        print("ğŸ“ æ‰§è¡Œè¯­ä¹‰åˆ†æ...")
        semantic_result = self.semantic_analysis(chat_data)
        
        # 2. è¡Œä¸ºæ¨¡å¼åˆ†æ
        print("ğŸ” æ‰§è¡Œè¡Œä¸ºæ¨¡å¼åˆ†æ...")
        behavioral_result = self.behavioral_analysis(chat_data)
        
        # 3. å…³ç³»ç½‘ç»œåˆ†æ
        print("ğŸ•¸ï¸  æ„å»ºå…³ç³»ç½‘ç»œ...")
        network_result = self.network_analyzer.analyze(chat_data)
        
        # 4. æ—¶é—´åºåˆ—åˆ†æ
        print("â° åˆ†ææ—¶é—´åºåˆ—...")
        temporal_result = self.temporal_analyzer.analyze(chat_data)
        
        # 5. ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æ
        print("ğŸ¯ æ‰§è¡Œä¸Šä¸‹æ–‡åˆ†æ...")
        context_result = self.context_analyzer.analyze(chat_data)
        
        # 6. ç»¼åˆé£é™©è¯„ä¼°
        print("ğŸ“Š è®¡ç®—ç»¼åˆé£é™©...")
        risk_assessment = self.assess_risk(
            semantic_result,
            behavioral_result,
            network_result,
            temporal_result,
            context_result
        )
        
        # 7. ç”Ÿæˆè¯æ®é“¾
        print("ğŸ”— æ„å»ºè¯æ®é“¾...")
        evidence_chain = self.build_evidence_chain(
            semantic_result,
            behavioral_result,
            network_result,
            temporal_result,
            context_result
        )
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        report = {
            "åˆ†ææ—¶é—´": datetime.now().isoformat(),
            "æ•°æ®æ¦‚è§ˆ": self.get_data_overview(chat_data),
            "è¯­ä¹‰åˆ†æ": semantic_result,
            "è¡Œä¸ºæ¨¡å¼": behavioral_result,
            "å…³ç³»ç½‘ç»œ": network_result,
            "æ—¶é—´åºåˆ—": temporal_result,
            "ä¸Šä¸‹æ–‡åˆ†æ": context_result,
            "é£é™©è¯„ä¼°": risk_assessment,
            "è¯æ®é“¾": evidence_chain,
            "å»ºè®®æªæ–½": self.generate_recommendations(risk_assessment)
        }
        
        print("âœ… åˆ†æå®Œæˆï¼")
        return report
    
    def semantic_analysis(self, chat_data: List[Dict]) -> Dict[str, Any]:
        """è¯­ä¹‰åˆ†æ - è¯†åˆ«éšæ™¦è¡¨è¾¾"""
        suspicious_messages = []
        pattern_matches = defaultdict(list)
        
        for msg in chat_data:
            content = msg.get("content", "")
            sender = msg.get("sender", "")
            timestamp = msg.get("timestamp", "")
            
            # æ£€æŸ¥æ¯ä¸ªè¯­ä¹‰æ¨¡å¼
            for pattern_type, patterns in self.semantic_patterns.items():
                for pattern in patterns:
                    if pattern in content:
                        match = {
                            "æ—¶é—´": timestamp,
                            "å‘é€è€…": sender,
                            "å†…å®¹": content,
                            "åŒ¹é…æ¨¡å¼": pattern,
                            "æ¨¡å¼ç±»å‹": pattern_type,
                            "ç½®ä¿¡åº¦": self._calculate_semantic_confidence(
                                content, pattern
                            )
                        }
                        suspicious_messages.append(match)
                        pattern_matches[pattern_type].append(match)
        
        return {
            "å¯ç–‘æ¶ˆæ¯æ•°": len(suspicious_messages),
            "æ¨¡å¼åŒ¹é…ç»Ÿè®¡": dict(pattern_matches),
            "è¯¦ç»†åŒ¹é…": suspicious_messages
        }
    
    def _calculate_semantic_confidence(self, content: str, pattern: str) -> float:
        """è®¡ç®—è¯­ä¹‰åŒ¹é…ç½®ä¿¡åº¦"""
        # åŸºç¡€ç½®ä¿¡åº¦
        base_confidence = 0.7
        
        # æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´
        confidence = base_confidence
        
        # å¦‚æœåŒ…å«å¤šä¸ªå¯ç–‘è¯ï¼Œæé«˜ç½®ä¿¡åº¦
        suspicious_count = sum(1 for p in self.semantic_patterns.values() 
                              if any(p2 in content for p2 in p))
        if suspicious_count > 1:
            confidence += 0.15
        
        # å¦‚æœæ¶ˆæ¯å¾ˆçŸ­ä¸”åŒ…å«å¯ç–‘è¯ï¼Œæé«˜ç½®ä¿¡åº¦
        if len(content) < 50 and pattern in content:
            confidence += 0.1
        
        return min(confidence, 0.99)
    
    def behavioral_analysis(self, chat_data: List[Dict]) -> Dict[str, Any]:
        """è¡Œä¸ºæ¨¡å¼åˆ†æ"""
        anomalies = []
        
        # 1. æ—¶é—´å¼‚å¸¸æ£€æµ‹
        time_anomalies = self._detect_time_anomalies(chat_data)
        anomalies.extend(time_anomalies)
        
        # 2. é¢‘ç‡å¼‚å¸¸æ£€æµ‹
        frequency_anomalies = self._detect_frequency_anomalies(chat_data)
        anomalies.extend(frequency_anomalies)
        
        # 3. ä¼šé¢æ¨¡å¼å¼‚å¸¸
        meeting_anomalies = self._detect_meeting_anomalies(chat_data)
        anomalies.extend(meeting_anomalies)
        
        # 4. è¯­è¨€æ¨¡å¼å¼‚å¸¸
        language_anomalies = self._detect_language_anomalies(chat_data)
        anomalies.extend(language_anomalies)
        
        return {
            "å¼‚å¸¸è¡Œä¸ºæ•°": len(anomalies),
            "å¼‚å¸¸ç±»å‹ç»Ÿè®¡": self._count_anomaly_types(anomalies),
            "è¯¦ç»†å¼‚å¸¸": anomalies
        }
    
    def _detect_time_anomalies(self, chat_data: List[Dict]) -> List[Dict]:
        """æ£€æµ‹æ—¶é—´å¼‚å¸¸"""
        anomalies = []
        
        for msg in chat_data:
            timestamp_str = msg.get("timestamp", "")
            sender = msg.get("sender", "")
            
            try:
                timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                hour = timestamp.hour
                weekday = timestamp.weekday()
                
                # æ·±å¤œèŠå¤©
                if hour >= 22 or hour < 6:
                    anomalies.append({
                        "ç±»å‹": "æ·±å¤œæ´»è·ƒ",
                        "æ—¶é—´": timestamp_str,
                        "å‚ä¸è€…": sender,
                        "ä¸¥é‡ç¨‹åº¦": "ä¸­"
                    })
                
                # å‘¨æœ«èŠå¤©
                if weekday >= 5:
                    anomalies.append({
                        "ç±»å‹": "å‘¨æœ«æ´»è·ƒ",
                        "æ—¶é—´": timestamp_str,
                        "å‚ä¸è€…": sender,
                        "ä¸¥é‡ç¨‹åº¦": "ä½"
                    })
            except:
                continue
        
        return anomalies
    
    def _detect_frequency_anomalies(self, chat_data: List[Dict]) -> List[Dict]:
        """æ£€æµ‹é¢‘ç‡å¼‚å¸¸"""
        anomalies = []
        
        # ç»Ÿè®¡æ¯ä¸ªå‚ä¸è€…çš„æ¶ˆæ¯é¢‘ç‡
        sender_counts = Counter(msg.get("sender", "") for msg in chat_data)
        total_messages = len(chat_data)
        avg_messages = total_messages / len(sender_counts) if sender_counts else 0
        
        for sender, count in sender_counts.items():
            # çªç„¶é«˜é¢‘
            if count > avg_messages * 3:
                anomalies.append({
                    "ç±»å‹": "é«˜é¢‘æ´»è·ƒ",
                    "å‚ä¸è€…": sender,
                    "æ¶ˆæ¯æ•°": count,
                    "å¹³å‡æ•°": avg_messages,
                    "ä¸¥é‡ç¨‹åº¦": "ä¸­"
                })
        
        return anomalies
    
    def _detect_meeting_anomalies(self, chat_data: List[Dict]) -> List[Dict]:
        """æ£€æµ‹ä¼šé¢å¼‚å¸¸"""
        anomalies = []
        
        # è¯†åˆ«ä¼šé¢ç›¸å…³å…³é”®è¯
        meeting_keywords = ["è§", "é¢", "èŠ", "èš", "åœ°æ–¹", "è€åœ°æ–¹"]
        
        for msg in chat_data:
            content = msg.get("content", "")
            if any(keyword in content for keyword in meeting_keywords):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç§ä¸‹ä¼šé¢
                if any(word in content for word in ["ç§ä¸‹", "å•ç‹¬", "ä¿å¯†", "åˆ«å‘Šè¯‰"]):
                    anomalies.append({
                        "ç±»å‹": "ç§ä¸‹ä¼šé¢",
                        "æ—¶é—´": msg.get("timestamp", ""),
                        "å‚ä¸è€…": msg.get("sender", ""),
                        "å†…å®¹": content,
                        "ä¸¥é‡ç¨‹åº¦": "é«˜"
                    })
        
        return anomalies
    
    def _detect_language_anomalies(self, chat_data: List[Dict]) -> List[Dict]:
        """æ£€æµ‹è¯­è¨€æ¨¡å¼å¼‚å¸¸"""
        anomalies = []
        
        for msg in chat_data:
            content = msg.get("content", "")
            sender = msg.get("sender", "")
            
            # æ£€æµ‹æ¨¡ç³ŠæŒ‡ä»£
            vague_words = ["é‚£ä¸ª", "è¿™ä¸ª", "é‚£ä¸ªä¸œè¥¿", "ä½ çŸ¥é“çš„", "éƒ½æ‡‚"]
            if sum(1 for word in vague_words if word in content) >= 2:
                anomalies.append({
                    "ç±»å‹": "æ¨¡ç³ŠæŒ‡ä»£",
                    "æ—¶é—´": msg.get("timestamp", ""),
                    "å‚ä¸è€…": sender,
                    "å†…å®¹": content,
                    "ä¸¥é‡ç¨‹åº¦": "ä¸­"
                })
            
            # æ£€æµ‹åˆ é™¤è®°å½•ç›¸å…³
            delete_keywords = ["åˆ é™¤", "æ’¤å›", "æ¸…ç†", "ä¸ç•™ç—•è¿¹"]
            if any(keyword in content for keyword in delete_keywords):
                anomalies.append({
                    "ç±»å‹": "è¯æ®é”€æ¯",
                    "æ—¶é—´": msg.get("timestamp", ""),
                    "å‚ä¸è€…": sender,
                    "å†…å®¹": content,
                    "ä¸¥é‡ç¨‹åº¦": "é«˜"
                })
        
        return anomalies
    
    def _count_anomaly_types(self, anomalies: List[Dict]) -> Dict[str, int]:
        """ç»Ÿè®¡å¼‚å¸¸ç±»å‹"""
        type_counts = Counter(anomaly.get("ç±»å‹", "æœªçŸ¥") for anomaly in anomalies)
        return dict(type_counts)
    
    def assess_risk(self, *analysis_results) -> Dict[str, Any]:
        """ç»¼åˆé£é™©è¯„ä¼°"""
        # è®¡ç®—é£é™©åˆ†æ•°
        risk_score = 0
        risk_factors = []
        
        semantic_result = analysis_results[0]
        behavioral_result = analysis_results[1]
        network_result = analysis_results[2]
        temporal_result = analysis_results[3]
        context_result = analysis_results[4]
        
        # 1. è¯­ä¹‰é£é™© (0-3åˆ†)
        semantic_risk = min(semantic_result.get("å¯ç–‘æ¶ˆæ¯æ•°", 0) / 5, 3)
        risk_score += semantic_risk
        if semantic_risk > 0:
            risk_factors.append(f"è¯­ä¹‰é£é™©: {semantic_risk:.1f}åˆ†")
        
        # 2. è¡Œä¸ºé£é™© (0-2åˆ†)
        behavioral_risk = min(behavioral_result.get("å¼‚å¸¸è¡Œä¸ºæ•°", 0) / 10, 2)
        risk_score += behavioral_risk
        if behavioral_risk > 0:
            risk_factors.append(f"è¡Œä¸ºé£é™©: {behavioral_risk:.1f}åˆ†")
        
        # 3. ç½‘ç»œé£é™© (0-2åˆ†)
        network_risk = min(len(network_result.get("å…³é”®äººç‰©", [])) / 3, 2)
        risk_score += network_risk
        if network_risk > 0:
            risk_factors.append(f"ç½‘ç»œé£é™©: {network_risk:.1f}åˆ†")
        
        # 4. æ—¶é—´é£é™© (0-2åˆ†)
        temporal_risk = min(len(temporal_result.get("å¼‚å¸¸æ—¶é—´ç‚¹", [])) / 5, 2)
        risk_score += temporal_risk
        if temporal_risk > 0:
            risk_factors.append(f"æ—¶é—´é£é™©: {temporal_risk:.1f}åˆ†")
        
        # 5. ä¸Šä¸‹æ–‡é£é™© (0-1åˆ†)
        context_risk = 0.5 if context_result.get("å‘ç°å¯ç–‘å…³è”", False) else 0
        risk_score += context_risk
        if context_risk > 0:
            risk_factors.append(f"ä¸Šä¸‹æ–‡é£é™©: {context_risk:.1f}åˆ†")
        
        # ç¡®å®šé£é™©ç­‰çº§
        risk_level = self._determine_risk_level(risk_score)
        
        return {
            "æ€»é£é™©åˆ†æ•°": round(risk_score, 2),
            "æœ€å¤§é£é™©åˆ†æ•°": 10,
            "é£é™©ç­‰çº§": risk_level,
            "é£é™©å› ç´ ": risk_factors,
            "ç½®ä¿¡åº¦": self._calculate_confidence(analysis_results)
        }
    
    def _determine_risk_level(self, score: float) -> str:
        """ç¡®å®šé£é™©ç­‰çº§"""
        if score >= 7:
            return "ğŸ”´ ä¸¥é‡é£é™©"
        elif score >= 5:
            return "ğŸŸ  é«˜é£é™©"
        elif score >= 3:
            return "ğŸŸ¡ ä¸­é£é™©"
        else:
            return "ğŸŸ¢ ä½é£é™©"
    
    def _calculate_confidence(self, analysis_results) -> float:
        """è®¡ç®—åˆ†æç½®ä¿¡åº¦"""
        # åŸºäºå¤šä¸ªåˆ†æç»“æœçš„ä¸€è‡´æ€§è®¡ç®—ç½®ä¿¡åº¦
        confidence = 0.7  # åŸºç¡€ç½®ä¿¡åº¦
        
        # å¦‚æœå¤šä¸ªåˆ†æéƒ½æŒ‡å‘é«˜é£é™©ï¼Œæé«˜ç½®ä¿¡åº¦
        high_risk_count = sum(
            1 for result in analysis_results
            if isinstance(result, dict) and 
               any("risk" in str(k).lower() or "å¼‚å¸¸" in str(k) or "å¯ç–‘" in str(k)
                   for k in result.keys())
        )
        
        if high_risk_count >= 3:
            confidence += 0.2
        
        return min(confidence, 0.99)
    
    def build_evidence_chain(self, *analysis_results) -> Dict[str, Any]:
        """æ„å»ºè¯æ®é“¾"""
        evidence = {
            "å®Œæ•´æ€§": "å®Œæ•´",
            "å…³é”®è¯æ®": [],
            "è¯æ®å¼ºåº¦": "å¼º"
        }
        
        # ä»å„ä¸ªåˆ†æç»“æœä¸­æå–å…³é”®è¯æ®
        for result in analysis_results:
            if isinstance(result, dict):
                # æå–è¯­ä¹‰è¯æ®
                if "è¯¦ç»†åŒ¹é…" in result:
                    for match in result["è¯¦ç»†åŒ¹é…"][:5]:  # å–å‰5ä¸ª
                        evidence["å…³é”®è¯æ®"].append({
                            "ç±»å‹": "è¯­ä¹‰è¯æ®",
                            "å†…å®¹": match.get("å†…å®¹", ""),
                            "æ—¶é—´": match.get("æ—¶é—´", ""),
                            "ç½®ä¿¡åº¦": match.get("ç½®ä¿¡åº¦", 0)
                        })
                
                # æå–è¡Œä¸ºè¯æ®
                if "è¯¦ç»†å¼‚å¸¸" in result:
                    for anomaly in result["è¯¦ç»†å¼‚å¸¸"][:5]:
                        evidence["å…³é”®è¯æ®"].append({
                            "ç±»å‹": "è¡Œä¸ºè¯æ®",
                            "å†…å®¹": anomaly.get("å†…å®¹", anomaly.get("ç±»å‹", "")),
                            "æ—¶é—´": anomaly.get("æ—¶é—´", ""),
                            "ä¸¥é‡ç¨‹åº¦": anomaly.get("ä¸¥é‡ç¨‹åº¦", "")
                        })
        
        return evidence
    
    def generate_recommendations(self, risk_assessment: Dict) -> List[str]:
        """ç”Ÿæˆå¤„ç†å»ºè®®"""
        risk_level = risk_assessment.get("é£é™©ç­‰çº§", "")
        recommendations = []
        
        if "ä¸¥é‡" in risk_level or "é«˜" in risk_level:
            recommendations = [
                "ç«‹å³å¯åŠ¨æ­£å¼è°ƒæŸ¥ç¨‹åº",
                "ä¿å…¨æ‰€æœ‰ç›¸å…³è¯æ®å’Œè®°å½•",
                "å¯¹å…³é”®äººç‰©è¿›è¡Œæ·±å…¥è°ƒæŸ¥",
                "æ£€æŸ¥ç›¸å…³ä¸šåŠ¡æµç¨‹å’Œå†³ç­–è®°å½•",
                "è€ƒè™‘æš‚åœç›¸å…³äººå‘˜çš„èŒåŠ¡æƒé™",
                "åè°ƒçºªæ£€ç›‘å¯Ÿéƒ¨é—¨ä»‹å…¥"
            ]
        elif "ä¸­" in risk_level:
            recommendations = [
                "åŠ å¼ºç›‘æ§å’Œå…³æ³¨",
                "æ”¶é›†æ›´å¤šè¯æ®ä¿¡æ¯",
                "è¿›è¡Œåˆæ­¥æ ¸å®",
                "æé†’ç›¸å…³äººå‘˜æ³¨æ„è¡Œä¸ºè§„èŒƒ"
            ]
        else:
            recommendations = [
                "ç»§ç»­ä¿æŒæ­£å¸¸ç›‘æ§",
                "å®šæœŸå¤æŸ¥"
            ]
        
        return recommendations
    
    def get_data_overview(self, chat_data: List[Dict]) -> Dict[str, Any]:
        """è·å–æ•°æ®æ¦‚è§ˆ"""
        if not chat_data:
            return {"æ€»æ¶ˆæ¯æ•°": 0}
        
        participants = set(msg.get("sender", "") for msg in chat_data)
        
        timestamps = []
        for msg in chat_data:
            try:
                ts = datetime.fromisoformat(
                    msg.get("timestamp", "").replace('Z', '+00:00')
                )
                timestamps.append(ts)
            except:
                continue
        
        time_range = {}
        if timestamps:
            time_range = {
                "å¼€å§‹æ—¶é—´": min(timestamps).isoformat(),
                "ç»“æŸæ—¶é—´": max(timestamps).isoformat(),
                "æ—¶é—´è·¨åº¦": str(max(timestamps) - min(timestamps))
            }
        
        return {
            "æ€»æ¶ˆæ¯æ•°": len(chat_data),
            "å‚ä¸äººæ•°": len(participants),
            "å‚ä¸è€…åˆ—è¡¨": list(participants),
            "æ—¶é—´èŒƒå›´": time_range
        }


class NetworkAnalyzer:
    """å…³ç³»ç½‘ç»œåˆ†æå™¨"""
    
    def analyze(self, chat_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå…³ç³»ç½‘ç»œ"""
        # æ„å»ºå…³ç³»å›¾
        relationships = self._build_relationships(chat_data)
        
        # è®¡ç®—ä¸­å¿ƒæ€§
        centrality = self._calculate_centrality(relationships)
        
        # è¯†åˆ«å…³é”®äººç‰©
        key_players = self._identify_key_players(centrality)
        
        # æ£€æµ‹å¼‚å¸¸è¿æ¥
        anomalous_connections = self._detect_anomalies(relationships)
        
        return {
            "å…³ç³»æ•°é‡": len(relationships),
            "å…³é”®äººç‰©": key_players,
            "ä¸­å¿ƒæ€§å¾—åˆ†": centrality,
            "å¼‚å¸¸è¿æ¥": anomalous_connections
        }
    
    def _build_relationships(self, chat_data: List[Dict]) -> List[Dict]:
        """æ„å»ºå…³ç³»"""
        relationships = []
        interaction_count = defaultdict(int)
        
        for msg in chat_data:
            sender = msg.get("sender", "")
            # ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰æ¶ˆæ¯éƒ½æ˜¯ç¾¤èŠæˆ–åŒå‘
            # å®é™…åº”è¯¥è§£ææ¥æ”¶è€…
            interaction_count[sender] += 1
        
        for person, count in interaction_count.items():
            relationships.append({
                "äººç‰©": person,
                "äº’åŠ¨æ¬¡æ•°": count,
                "æ´»è·ƒåº¦": "é«˜" if count > 10 else "ä¸­" if count > 5 else "ä½"
            })
        
        return relationships
    
    def _calculate_centrality(self, relationships: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—ä¸­å¿ƒæ€§"""
        centrality = {}
        max_count = max((r["äº’åŠ¨æ¬¡æ•°"] for r in relationships), default=1)
        
        for rel in relationships:
            person = rel["äººç‰©"]
            count = rel["äº’åŠ¨æ¬¡æ•°"]
            centrality[person] = count / max_count if max_count > 0 else 0
        
        return centrality
    
    def _identify_key_players(self, centrality: Dict[str, float]) -> List[str]:
        """è¯†åˆ«å…³é”®äººç‰©"""
        # æŒ‰ä¸­å¿ƒæ€§æ’åº
        sorted_people = sorted(
            centrality.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # è¿”å›å‰3å
        return [person for person, score in sorted_people[:3]]
    
    def _detect_anomalies(self, relationships: List[Dict]) -> List[str]:
        """æ£€æµ‹å¼‚å¸¸è¿æ¥"""
        anomalies = []
        
        # æ£€æµ‹å¼‚å¸¸é«˜çš„æ´»è·ƒåº¦
        for rel in relationships:
            if rel["äº’åŠ¨æ¬¡æ•°"] > 20:
                anomalies.append(
                    f"{rel['äººç‰©']} æ´»è·ƒåº¦å¼‚å¸¸é«˜ ({rel['äº’åŠ¨æ¬¡æ•°']}æ¬¡)"
                )
        
        return anomalies


class TemporalAnalyzer:
    """æ—¶é—´åºåˆ—åˆ†æå™¨"""
    
    def analyze(self, chat_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´åºåˆ—"""
        # æ„å»ºæ—¶é—´çº¿
        timeline = self._build_timeline(chat_data)
        
        # æ£€æµ‹å¼‚å¸¸æ—¶é—´ç‚¹
        anomalies = self._detect_temporal_anomalies(chat_data)
        
        # åˆ†ææ—¶é—´æ¨¡å¼
        patterns = self._analyze_temporal_patterns(chat_data)
        
        return {
            "æ—¶é—´çº¿äº‹ä»¶": len(timeline),
            "å¼‚å¸¸æ—¶é—´ç‚¹": anomalies,
            "æ—¶é—´æ¨¡å¼": patterns
        }
    
    def _build_timeline(self, chat_data: List[Dict]) -> List[Dict]:
        """æ„å»ºæ—¶é—´çº¿"""
        timeline = []
        for msg in chat_data:
            timeline.append({
                "æ—¶é—´": msg.get("timestamp", ""),
                "äº‹ä»¶": msg.get("content", "")[:50]  # å‰50ä¸ªå­—ç¬¦
            })
        return timeline
    
    def _detect_temporal_anomalies(self, chat_data: List[Dict]) -> List[str]:
        """æ£€æµ‹æ—¶é—´å¼‚å¸¸"""
        anomalies = []
        
        for msg in chat_data:
            timestamp_str = msg.get("timestamp", "")
            try:
                timestamp = datetime.fromisoformat(
                    timestamp_str.replace('Z', '+00:00')
                )
                hour = timestamp.hour
                
                # æ·±å¤œæ¶ˆæ¯
                if hour >= 22 or hour < 6:
                    anomalies.append(
                        f"æ·±å¤œæ¶ˆæ¯: {timestamp_str}"
                    )
            except:
                continue
        
        return anomalies[:10]  # æœ€å¤šè¿”å›10ä¸ª
    
    def _analyze_temporal_patterns(self, chat_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        hour_counts = Counter()
        weekday_counts = Counter()
        
        for msg in chat_data:
            timestamp_str = msg.get("timestamp", "")
            try:
                timestamp = datetime.fromisoformat(
                    timestamp_str.replace('Z', '+00:00')
                )
                hour_counts[timestamp.hour] += 1
                weekday_counts[timestamp.weekday()] += 1
            except:
                continue
        
        return {
            "å°æ—¶åˆ†å¸ƒ": dict(hour_counts),
            "æ˜ŸæœŸåˆ†å¸ƒ": dict(weekday_counts)
        }


class ContextAwareAnalyzer:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æå™¨"""
    
    def analyze(self, chat_data: List[Dict]) -> Dict[str, Any]:
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æ"""
        # æ£€æµ‹å¯ç–‘å…³è”
        suspicious_associations = self._detect_suspicious_associations(chat_data)
        
        # åˆ†æå¯¹è¯ä¸Šä¸‹æ–‡
        context_patterns = self._analyze_context_patterns(chat_data)
        
        return {
            "å‘ç°å¯ç–‘å…³è”": len(suspicious_associations) > 0,
            "å¯ç–‘å…³è”è¯¦æƒ…": suspicious_associations,
            "ä¸Šä¸‹æ–‡æ¨¡å¼": context_patterns
        }
    
    def _detect_suspicious_associations(self, chat_data: List[Dict]) -> List[Dict]:
        """æ£€æµ‹å¯ç–‘å…³è”"""
        associations = []
        
        # æ£€æµ‹ç‰¹å®šè¯é¢˜çš„é¢‘ç¹å‡ºç°
        topic_keywords = {
            "é¡¹ç›®": ["é¡¹ç›®", "æ‹›æ ‡", "é‡‡è´­", "åˆåŒ"],
            "èµ„é‡‘": ["é’±", "æ¬¾", "è´¹ç”¨", "é¢„ç®—"],
            "ä¼šé¢": ["è§", "é¢", "èŠ", "èš"]
        }
        
        for topic, keywords in topic_keywords.items():
            count = sum(
                1 for msg in chat_data
                if any(keyword in msg.get("content", "") for keyword in keywords)
            )
            if count > 5:
                associations.append({
                    "è¯é¢˜": topic,
                    "å‡ºç°æ¬¡æ•°": count,
                    "å¼‚å¸¸": "é«˜"
                })
        
        return associations
    
    def _analyze_context_patterns(self, chat_data: List[Dict]) -> List[str]:
        """åˆ†æä¸Šä¸‹æ–‡æ¨¡å¼"""
        patterns = []
        
        # æ£€æµ‹è¯é¢˜è½¬æ¢
        if len(chat_data) > 10:
            patterns.append("å­˜åœ¨å¤šè¯é¢˜è®¨è®º")
        
        # æ£€æµ‹å‚ä¸è€…å˜åŒ–
        participants = set(msg.get("sender", "") for msg in chat_data)
        if len(participants) > 2:
            patterns.append("å¤šäººå‚ä¸è®¨è®º")
        
        return patterns


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python advanced_analyzer.py <input_file> <output_file>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2]
    
    # è¯»å–èŠå¤©è®°å½•
    print(f"ğŸ“‚ è¯»å–æ–‡ä»¶: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        chat_data = json.load(f)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = AdvancedCorruptionAnalyzer()
    
    # æ‰§è¡Œåˆ†æ
    report = analyzer.analyze(chat_data)
    
    # ä¿å­˜æŠ¥å‘Š
    print(f"ğŸ’¾ ä¿å­˜æŠ¥å‘Š: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ“Š åˆ†ææ‘˜è¦")
    print("="*50)
    print(f"é£é™©ç­‰çº§: {report['é£é™©è¯„ä¼°']['é£é™©ç­‰çº§']}")
    print(f"é£é™©åˆ†æ•°: {report['é£é™©è¯„ä¼°']['æ€»é£é™©åˆ†æ•°']}/{report['é£é™©è¯„ä¼°']['æœ€å¤§é£é™©åˆ†æ•°']}")
    print(f"ç½®ä¿¡åº¦: {report['é£é™©è¯„ä¼°']['ç½®ä¿¡åº¦']*100:.1f}%")
    print(f"\nå…³é”®å‘ç°:")
    for factor in report['é£é™©è¯„ä¼°']['é£é™©å› ç´ ']:
        print(f"  - {factor}")
    print(f"\nå»ºè®®æªæ–½:")
    for i, rec in enumerate(report['å»ºè®®æªæ–½'], 1):
        print(f"  {i}. {rec}")
    print("="*50)


if __name__ == "__main__":
    main()
