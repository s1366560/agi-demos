#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åè…è´¥è°ƒæŸ¥æŠ€èƒ½ v3.0 - å¯æ‰©å±•åˆ†æå¼•æ“
æ”¯æŒç™¾ä¸‡é‡çº§èŠå¤©è®°å½•çš„é«˜æ•ˆåˆ†æ

æ ¸å¿ƒç‰¹æ€§:
1. æµå¼å¤„ç† - é¿å…å†…å­˜æº¢å‡º
2. å¹¶è¡Œè®¡ç®— - å……åˆ†åˆ©ç”¨å¤šæ ¸CPU
3. å¢é‡æ›´æ–° - åªå¤„ç†æ–°æ•°æ®
4. æ™ºèƒ½ç¼“å­˜ - é¿å…é‡å¤è®¡ç®—
5. ç´¢å¼•ä¼˜åŒ– - å¿«é€ŸæŸ¥è¯¢æ£€ç´¢

ä½œè€…: åè…è´¥è°ƒæŸ¥æŠ€èƒ½å›¢é˜Ÿ
ç‰ˆæœ¬: 3.0.0
æ—¥æœŸ: 2026-02-09
"""

import json
import os
import re
import time
import pickle
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Iterator, Optional, Tuple
from pathlib import Path
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import Pool, cpu_count
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MessageIndex:
    """é«˜æ€§èƒ½æ¶ˆæ¯ç´¢å¼•"""
    
    def __init__(self):
        self.timestamp_index = {}  # æ—¶é—´ç´¢å¼•
        self.sender_index = {}     # å‘é€è€…ç´¢å¼•
        self.keyword_index = {}    # å…³é”®è¯ç´¢å¼•
        self.risk_index = {}       # é£é™©ç´¢å¼•
        self._built = False
    
    def build(self, messages: List[Dict]) -> None:
        """æ„å»ºç´¢å¼•"""
        logger.info(f"å¼€å§‹æ„å»ºç´¢å¼•ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
        start_time = time.time()
        
        for idx, msg in enumerate(messages):
            # æ—¶é—´ç´¢å¼• (æŒ‰å¤©)
            ts = msg.get('timestamp', '')
            date_key = ts[:10] if ts else 'unknown'
            if date_key not in self.timestamp_index:
                self.timestamp_index[date_key] = []
            self.timestamp_index[date_key].append(idx)
            
            # å‘é€è€…ç´¢å¼•
            sender = msg.get('sender', 'unknown')
            if sender not in self.sender_index:
                self.sender_index[sender] = []
            self.sender_index[sender].append(idx)
            
            # å…³é”®è¯ç´¢å¼• (æå–ä¸­æ–‡è¯æ±‡)
            content = msg.get('content', '')
            keywords = self._extract_keywords(content)
            for keyword in keywords:
                if keyword not in self.keyword_index:
                    self.keyword_index[keyword] = []
                self.keyword_index[keyword].append(idx)
        
        self._built = True
        elapsed = time.time() - start_time
        logger.info(f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
        logger.info(f"  - æ—¶é—´åˆ†åŒº: {len(self.timestamp_index)}")
        logger.info(f"  - å‘é€è€…: {len(self.sender_index)}")
        logger.info(f"  - å…³é”®è¯: {len(self.keyword_index)}")
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆå®é™…åº”ç”¨ä¸­å¯ç”¨jiebaï¼‰
        keywords = []
        
        # æå–2-4å­—çš„è¯è¯­
        for i in range(len(text)):
            for length in [2, 3, 4]:
                if i + length <= len(text):
                    word = text[i:i+length]
                    if self._is_meaningful_word(word):
                        keywords.append(word)
        
        return keywords[:10]  # é™åˆ¶å…³é”®è¯æ•°é‡
    
    def _is_meaningful_word(self, word: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æœ‰æ„ä¹‰çš„è¯"""
        # è¿‡æ»¤æ‰çº¯æ•°å­—ã€çº¯ç¬¦å·ç­‰
        if not word:
            return False
        if word.isdigit():
            return False
        if all(c in 'ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹' for c in word):
            return False
        return True
    
    def query_by_sender(self, sender: str) -> List[int]:
        """æŒ‰å‘é€è€…æŸ¥è¯¢"""
        return self.sender_index.get(sender, [])
    
    def query_by_date(self, date: str) -> List[int]:
        """æŒ‰æ—¥æœŸæŸ¥è¯¢"""
        return self.timestamp_index.get(date, [])
    
    def query_by_keyword(self, keyword: str) -> List[int]:
        """æŒ‰å…³é”®è¯æŸ¥è¯¢"""
        return self.keyword_index.get(keyword, [])


class AnalysisCache:
    """åˆ†æç»“æœç¼“å­˜"""
    
    def __init__(self, cache_dir: str = 'cache'):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.memory_cache = {}
        self.hit_count = 0
        self.miss_count = 0
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        # å…ˆæŸ¥å†…å­˜ç¼“å­˜
        if key in self.memory_cache:
            self.hit_count += 1
            return self.memory_cache[key]
        
        # å†æŸ¥ç£ç›˜ç¼“å­˜
        cache_file = self.cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                data = pickle.load(f)
                self.memory_cache[key] = data
                self.hit_count += 1
                return data
        
        self.miss_count += 1
        return None
    
    def set(self, key: str, value: Any) -> None:
        """è®¾ç½®ç¼“å­˜"""
        # ä¿å­˜åˆ°å†…å­˜
        self.memory_cache[key] = value
        
        # ä¿å­˜åˆ°ç£ç›˜
        cache_file = self.cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total if total > 0 else 0
        return {
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': hit_rate,
            'memory_size': len(self.memory_cache)
        }


class ScalableAnalyzer:
    """å¯æ‰©å±•åˆ†æå¼•æ“ - æ”¯æŒç™¾ä¸‡é‡çº§æ•°æ®"""
    
    # éšæ™¦è…è´¥æ¨¡å¼åº“
    CORRUPTION_PATTERNS = {
        'financial': [
            r'é‚£ç¬”.*?é’±',
            r'é‚£ä¸ª.*?ä¸œè¥¿',
            r'è€è§„çŸ©',
            r'æ„æ€ä¸€ä¸‹',
            r'è¡¨ç¤º.*?å¿ƒæ„',
            r'è¾›è‹¦è´¹',
            r'èŒ¶æ°´è´¹',
            r'æ‰“ç‚¹',
        ],
        'meeting': [
            r'è€åœ°æ–¹',
            r'ç§ä¸‹.*?è§',
            r'å•ç‹¬.*?èŠ',
            r'ä¿å¯†',
            r'åˆ«å‘Šè¯‰.*?äºº',
            r'åªæœ‰.*?çŸ¥é“',
        ],
        'abuse': [
            r'ç‰¹æ®Š.*?ç…§é¡¾',
            r'é€šè.*?ä¸‹',
            r'æŒ‰.*?è§„çŸ©',
            r'ç ´ä¾‹',
            r'å¼€.*?ç»¿ç¯',
            r'å…³ç…§',
        ],
        'evidence': [
            r'åˆ é™¤.*?è®°å½•',
            r'æ¸…ç†.*?èŠå¤©',
            r'ä¸ç•™.*?ç—•è¿¹',
            r'æ’¤å›',
            r'æ¯æ‰',
        ]
    }
    
    def __init__(self, 
                 batch_size: int = 10000,
                 workers: int = None,
                 enable_cache: bool = True,
                 cache_dir: str = 'cache'):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°
            workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_dir: ç¼“å­˜ç›®å½•
        """
        self.batch_size = batch_size
        self.workers = workers or cpu_count()
        self.enable_cache = enable_cache
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.cache = AnalysisCache(cache_dir) if enable_cache else None
        self.index = MessageIndex()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_messages': 0,
            'processed_messages': 0,
            'suspicious_messages': 0,
            'start_time': None,
            'end_time': None
        }
        
        logger.info(f"åˆå§‹åŒ–å¯æ‰©å±•åˆ†æå¼•æ“")
        logger.info(f"  - æ‰¹å¤„ç†å¤§å°: {batch_size}")
        logger.info(f"  - å·¥ä½œè¿›ç¨‹: {self.workers}")
        logger.info(f"  - ç¼“å­˜: {'å¯ç”¨' if enable_cache else 'ç¦ç”¨'}")
    
    def analyze_large_dataset(self, 
                             input_path: str,
                             output_path: str = None,
                             sample_rate: float = 1.0) -> Dict[str, Any]:
        """
        åˆ†æå¤§è§„æ¨¡æ•°æ®é›†
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            sample_rate: é‡‡æ ·ç‡ (0.0-1.0)
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        logger.info(f"å¼€å§‹åˆ†æå¤§è§„æ¨¡æ•°æ®é›†: {input_path}")
        self.stats['start_time'] = time.time()
        
        # ç¬¬ä¸€é˜¶æ®µ: æµå¼è¯»å–å’Œé‡‡æ ·
        logger.info("ç¬¬ä¸€é˜¶æ®µ: æµå¼è¯»å–å’Œé‡‡æ ·...")
        messages = list(self._stream_read(input_path, sample_rate))
        self.stats['total_messages'] = len(messages)
        logger.info(f"  è¯»å–æ¶ˆæ¯: {len(messages)} æ¡")
        
        # ç¬¬äºŒé˜¶æ®µ: æ„å»ºç´¢å¼•
        logger.info("ç¬¬äºŒé˜¶æ®µ: æ„å»ºç´¢å¼•...")
        self.index.build(messages)
        
        # ç¬¬ä¸‰é˜¶æ®µ: å¹¶è¡Œåˆ†æ
        logger.info("ç¬¬ä¸‰é˜¶æ®µ: å¹¶è¡Œåˆ†æ...")
        results = self._parallel_analyze(messages)
        
        # ç¬¬å››é˜¶æ®µ: å…³ç³»ç½‘ç»œåˆ†æ
        logger.info("ç¬¬å››é˜¶æ®µ: å…³ç³»ç½‘ç»œåˆ†æ...")
        network = self._build_network(results)
        
        # ç¬¬äº”é˜¶æ®µ: é£é™©è¯„ä¼°
        logger.info("ç¬¬äº”é˜¶æ®µ: é£é™©è¯„ä¼°...")
        risk_assessment = self._assess_risk(results, network)
        
        # æ±‡æ€»ç»“æœ
        self.stats['end_time'] = time.time()
        self.stats['processed_messages'] = len(messages)
        self.stats['suspicious_messages'] = len(results)
        
        final_report = {
            'metadata': {
                'analysis_time': datetime.now().isoformat(),
                'elapsed_time': self.stats['end_time'] - self.stats['start_time'],
                'total_messages': self.stats['total_messages'],
                'suspicious_messages': self.stats['suspicious_messages'],
                'sample_rate': sample_rate
            },
            'suspicious_messages': results,
            'network_analysis': network,
            'risk_assessment': risk_assessment,
            'performance_stats': self._get_performance_stats()
        }
        
        # ä¿å­˜ç»“æœ
        if output_path:
            self._save_report(final_report, output_path)
            logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(final_report)
        
        return final_report
    
    def _stream_read(self, 
                    file_path: str, 
                    sample_rate: float = 1.0) -> Iterator[Dict]:
        """
        æµå¼è¯»å–æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            sample_rate: é‡‡æ ·ç‡
        
        Yields:
            æ¶ˆæ¯å­—å…¸
        """
        import random
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                # é‡‡æ ·
                if sample_rate < 1.0 and random.random() > sample_rate:
                    continue
                
                try:
                    message = json.loads(line.strip())
                    yield message
                except json.JSONDecodeError:
                    continue
    
    def _parallel_analyze(self, messages: List[Dict]) -> List[Dict]:
        """
        å¹¶è¡Œåˆ†ææ¶ˆæ¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            å¯ç–‘æ¶ˆæ¯åˆ—è¡¨
        """
        # åˆ†æ‰¹
        batches = [messages[i:i+self.batch_size] 
                  for i in range(0, len(messages), self.batch_size)]
        
        logger.info(f"åˆ†æˆ {len(batches)} ä¸ªæ‰¹æ¬¡å¹¶è¡Œå¤„ç†...")
        
        suspicious_results = []
        
        # å¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=self.workers) as executor:
            futures = [executor.submit(self._analyze_batch, batch) 
                      for batch in batches]
            
            for idx, future in enumerate(as_completed(futures)):
                batch_results = future.result()
                suspicious_results.extend(batch_results)
                
                if (idx + 1) % 10 == 0:
                    logger.info(f"  å·²å®Œæˆ {idx + 1}/{len(batches)} æ‰¹æ¬¡")
        
        return suspicious_results
    
    @staticmethod
    def _analyze_batch(batch: List[Dict]) -> List[Dict]:
        """
        åˆ†æå•ä¸ªæ‰¹æ¬¡
        
        Args:
            batch: æ¶ˆæ¯æ‰¹æ¬¡
        
        Returns:
            å¯ç–‘æ¶ˆæ¯åˆ—è¡¨
        """
        results = []
        
        for message in batch:
            # æ£€æŸ¥æ˜¯å¦å¯ç–‘
            suspicion = ScalableAnalyzer._check_suspicion(message)
            if suspicion['is_suspicious']:
                results.append({
                    **message,
                    'suspicion_analysis': suspicion
                })
        
        return results
    
    @staticmethod
    def _check_suspicion(message: Dict) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ¶ˆæ¯å¯ç–‘æ€§
        
        Args:
            message: æ¶ˆæ¯å­—å…¸
        
        Returns:
            å¯ç–‘æ€§åˆ†æç»“æœ
        """
        content = message.get('content', '')
        sender = message.get('sender', '')
        timestamp = message.get('timestamp', '')
        
        detected_patterns = []
        confidence = 0.0
        
        # æ£€æŸ¥å„ç§è…è´¥æ¨¡å¼
        for category, patterns in ScalableAnalyzer.CORRUPTION_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, content):
                    detected_patterns.append({
                        'category': category,
                        'pattern': pattern,
                        'matched_text': re.findall(pattern, content)
                    })
                    confidence += 0.15
        
        # æ£€æŸ¥æ—¶é—´å¼‚å¸¸
        time_anomaly = ScalableAnalyzer._check_time_anomaly(timestamp)
        if time_anomaly['is_anomaly']:
            confidence += 0.1
            detected_patterns.append({
                'category': 'time_anomaly',
                'description': time_anomaly['reason']
            })
        
        # é™åˆ¶ç½®ä¿¡åº¦èŒƒå›´
        confidence = min(confidence, 1.0)
        
        return {
            'is_suspicious': confidence > 0.3,
            'confidence': confidence,
            'detected_patterns': detected_patterns,
            'risk_level': ScalableAnalyzer._get_risk_level(confidence)
        }
    
    @staticmethod
    def _check_time_anomaly(timestamp: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ—¶é—´å¼‚å¸¸"""
        if not timestamp:
            return {'is_anomaly': False}
        
        try:
            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            hour = dt.hour
            weekend = dt.weekday() >= 5
            
            # æ·±å¤œ (22:00-06:00)
            if hour >= 22 or hour < 6:
                return {
                    'is_anomaly': True,
                    'reason': 'æ·±å¤œèŠå¤©'
                }
            
            # å‘¨æœ«
            if weekend:
                return {
                    'is_anomaly': True,
                    'reason': 'å‘¨æœ«èŠå¤©'
                }
            
        except:
            pass
        
        return {'is_anomaly': False}
    
    @staticmethod
    def _get_risk_level(confidence: float) -> str:
        """è·å–é£é™©ç­‰çº§"""
        if confidence >= 0.7:
            return 'HIGH'
        elif confidence >= 0.4:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _build_network(self, results: List[Dict]) -> Dict[str, Any]:
        """æ„å»ºå…³ç³»ç½‘ç»œ"""
        network = defaultdict(lambda: {'connections': set(), 'count': 0})
        
        for result in results:
            sender = result.get('sender', '')
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å…³ç³»åˆ†æ
            network[sender]['count'] += 1
        
        # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
        return {
            node: {
                'connections': list(data['connections']),
                'message_count': data['count']
            }
            for node, data in network.items()
        }
    
    def _assess_risk(self, 
                    results: List[Dict], 
                    network: Dict) -> Dict[str, Any]:
        """è¯„ä¼°æ•´ä½“é£é™©"""
        if not results:
            return {
                'overall_risk': 'LOW',
                'risk_score': 0.0,
                'factors': []
            }
        
        # è®¡ç®—é£é™©åˆ†æ•°
        high_risk_count = sum(1 for r in results 
                             if r.get('suspicion_analysis', {}).get('risk_level') == 'HIGH')
        medium_risk_count = sum(1 for r in results 
                               if r.get('suspicion_analysis', {}).get('risk_level') == 'MEDIUM')
        
        risk_score = (high_risk_count * 1.0 + medium_risk_count * 0.5) / len(results)
        risk_score = min(risk_score * 10, 10)  # è½¬æ¢åˆ°0-10åˆ†
        
        # ç¡®å®šé£é™©ç­‰çº§
        if risk_score >= 7:
            overall_risk = 'HIGH'
        elif risk_score >= 4:
            overall_risk = 'MEDIUM'
        else:
            overall_risk = 'LOW'
        
        return {
            'overall_risk': overall_risk,
            'risk_score': risk_score,
            'high_risk_count': high_risk_count,
            'medium_risk_count': medium_risk_count,
            'total_suspicious': len(results),
            'factors': [
                f"é«˜é£é™©æ¶ˆæ¯: {high_risk_count} æ¡",
                f"ä¸­é£é™©æ¶ˆæ¯: {medium_risk_count} æ¡",
                f"æ€»å¯ç–‘æ¶ˆæ¯: {len(results)} æ¡"
            ]
        }
    
    def _get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        elapsed = self.stats['end_time'] - self.stats['start_time']
        throughput = self.stats['total_messages'] / elapsed if elapsed > 0 else 0
        
        stats = {
            'elapsed_time': elapsed,
            'throughput_per_second': throughput,
            'workers_used': self.workers,
            'batch_size': self.batch_size
        }
        
        if self.cache:
            stats['cache_stats'] = self.cache.get_stats()
        
        return stats
    
    def _save_report(self, report: Dict, output_path: str) -> None:
        """ä¿å­˜æŠ¥å‘Š"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
    
    def _print_summary(self, report: Dict) -> None:
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*80)
        print("åè…è´¥è°ƒæŸ¥åˆ†ææŠ¥å‘Š".center(80))
        print("="*80)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        print(f"  åˆ†ææ—¶é—´: {report['metadata']['analysis_time']}")
        print(f"  æ€»æ¶ˆæ¯æ•°: {report['metadata']['total_messages']:,} æ¡")
        print(f"  å¯ç–‘æ¶ˆæ¯: {report['metadata']['suspicious_messages']:,} æ¡")
        print(f"  é‡‡æ ·ç‡: {report['metadata']['sample_rate']:.1%}")
        
        # æ€§èƒ½æŒ‡æ ‡
        perf = report['performance_stats']
        print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        print(f"  å¤„ç†æ—¶é—´: {perf['elapsed_time']:.2f} ç§’")
        print(f"  ååé‡: {perf['throughput_per_second']:.1f} æ¡/ç§’")
        print(f"  å·¥ä½œè¿›ç¨‹: {perf['workers_used']}")
        
        if 'cache_stats' in perf:
            cache = perf['cache_stats']
            print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache['hit_rate']:.1%}")
        
        # é£é™©è¯„ä¼°
        risk = report['risk_assessment']
        print(f"\nâš ï¸  é£é™©è¯„ä¼°:")
        print(f"  æ•´ä½“é£é™©: {risk['overall_risk']}")
        print(f"  é£é™©åˆ†æ•°: {risk['risk_score']:.1f}/10")
        print(f"  é«˜é£é™©: {risk['high_risk_count']} æ¡")
        print(f"  ä¸­é£é™©: {risk['medium_risk_count']} æ¡")
        
        # å…³é”®å‘ç°
        if report['suspicious_messages']:
            print(f"\nğŸ” å…³é”®å‘ç°:")
            high_risk = [m for m in report['suspicious_messages'] 
                        if m.get('suspicion_analysis', {}).get('risk_level') == 'HIGH']
            for msg in high_risk[:5]:
                sender = msg.get('sender', 'Unknown')
                content = msg.get('content', '')[:50]
                confidence = msg.get('suspicion_analysis', {}).get('confidence', 0)
                print(f"  - [{sender}] {content}... (ç½®ä¿¡åº¦: {confidence:.1%})")
        
        print("\n" + "="*80 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python scalable_analyzer.py <input_file> [output_file] [sample_rate]")
        print("ç¤ºä¾‹: python scalable_analyzer.py data/messages.json report.json 0.1")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    sample_rate = float(sys.argv[3]) if len(sys.argv) > 3 else 1.0
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ScalableAnalyzer(
        batch_size=10000,
        workers=cpu_count(),
        enable_cache=True
    )
    
    # æ‰§è¡Œåˆ†æ
    results = analyzer.analyze_large_dataset(
        input_path=input_file,
        output_path=output_file,
        sample_rate=sample_rate
    )


if __name__ == '__main__':
    main()
