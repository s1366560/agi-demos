#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åè…è´¥è°ƒæŸ¥æŠ€èƒ½ v4.0 - å…³ç³»ç½‘ç»œåˆ†æå™¨
Anti-Corruption Investigation Skill v4.0 - Relationship Network Analyzer

ä¸“é—¨ç”¨äºåˆ†æèŠå¤©è®°å½•ä¸­çš„äººç‰©å…³ç³»ç½‘ç»œï¼Œè¯†åˆ«è…è´¥å›¢ä¼™å’Œå…³é”®äººç‰©
"""

import json
import networkx as nx
from collections import defaultdict, Counter
from datetime import datetime
from typing import Dict, List, Tuple, Any
import numpy as np
from community import community_louvain
from scipy import spatial


class RelationshipAnalyzer:
    """å…³ç³»ç½‘ç»œåˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.graph = None
        self.messages = []
        self.participants = set()
        
    def load_messages(self, file_path: str) -> List[Dict]:
        """åŠ è½½èŠå¤©æ¶ˆæ¯
        
        Args:
            file_path: JSONLæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            msg = json.loads(line)
                            messages.append(msg)
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            print(f"åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
            return []
            
        self.messages = messages
        print(f"âœ… æˆåŠŸåŠ è½½ {len(messages)} æ¡æ¶ˆæ¯")
        return messages
    
    def build_network(self, messages: List[Dict]) -> nx.Graph:
        """æ„å»ºå…³ç³»ç½‘ç»œ
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            NetworkXå›¾å¯¹è±¡
        """
        G = nx.Graph()
        
        # ç»Ÿè®¡äº¤äº’é¢‘ç‡å’Œç±»å‹
        interactions = defaultdict(lambda: {
            'count': 0,
            'types': Counter(),
            'time_patterns': [],
            'suspicious_count': 0
        })
        
        # æå–æ‰€æœ‰å‚ä¸è€…
        participants = set()
        for msg in messages:
            sender = msg.get('sender', '')
            receiver = msg.get('receiver', '')
            participants.add(sender)
            if receiver:
                participants.add(receiver)
        
        self.participants = participants
        
        # åˆ†ææ¯æ¡æ¶ˆæ¯
        for msg in messages:
            sender = msg.get('sender', '')
            receiver = msg.get('receiver', '')
            content = msg.get('content', '')
            timestamp = msg.get('timestamp', '')
            
            # å¦‚æœæœ‰æ˜ç¡®çš„æ¥æ”¶è€…ï¼Œç›´æ¥å»ºç«‹è¿æ¥
            if receiver and receiver != sender:
                key = tuple(sorted([sender, receiver]))
                interactions[key]['count'] += 1
                
                # åˆ†æäº¤äº’ç±»å‹
                interaction_type = self._classify_interaction(content)
                interactions[key]['types'][interaction_type] += 1
                
                # è®°å½•æ—¶é—´æ¨¡å¼
                if timestamp:
                    try:
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        hour = dt.hour
                        interactions[key]['time_patterns'].append(hour)
                    except:
                        pass
                
                # æ£€æµ‹å¯ç–‘è¡Œä¸º
                if self._is_suspicious_interaction(content):
                    interactions[key]['suspicious_count'] += 1
            else:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®æ¥æ”¶è€…ï¼Œå°è¯•ä»å†…å®¹ä¸­æåŠçš„äºº
                mentioned = self._extract_mentions(content, participants)
                for mentioned_person in mentioned:
                    if mentioned_person != sender:
                        key = tuple(sorted([sender, mentioned_person]))
                        interactions[key]['count'] += 1
                        
                        # åˆ†æäº¤äº’ç±»å‹
                        interaction_type = self._classify_interaction(content)
                        interactions[key]['types'][interaction_type] += 1
                        
                        # è®°å½•æ—¶é—´æ¨¡å¼
                        if timestamp:
                            try:
                                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                hour = dt.hour
                                interactions[key]['time_patterns'].append(hour)
                            except:
                                pass
                        
                        # æ£€æµ‹å¯ç–‘è¡Œä¸º
                        if self._is_suspicious_interaction(content):
                            interactions[key]['suspicious_count'] += 1
        
        # æ„å»ºå›¾
        for (person1, person2), data in interactions.items():
            if person1 in participants and person2 in participants:
                # è®¡ç®—æƒé‡
                weight = data['count']
                
                # è®¡ç®—å¯ç–‘åº¦
                suspicious_ratio = data['suspicious_count'] / max(data['count'], 1)
                
                # æ·»åŠ è¾¹
                G.add_edge(person1, person2, 
                          weight=weight,
                          suspicious_ratio=suspicious_ratio,
                          interaction_types=dict(data['types']),
                          suspicious_count=data['suspicious_count'])
        
        self.graph = G
        print(f"âœ… æ„å»ºå…³ç³»ç½‘ç»œ: {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹")
        return G
    
    def _extract_mentions(self, content: str, participants: set) -> List[str]:
        """æå–æ¶ˆæ¯ä¸­æåˆ°çš„äºº
        
        Args:
            content: æ¶ˆæ¯å†…å®¹
            participants: å‚ä¸è€…åˆ—è¡¨
            
        Returns:
            æåˆ°çš„äººååˆ—è¡¨
        """
        mentioned = []
        
        # ç›´æ¥æåŠ
        for person in participants:
            if person in content:
                mentioned.append(person)
        
        return mentioned
    
    def _classify_interaction(self, content: str) -> str:
        """åˆ†ç±»äº¤äº’ç±»å‹
        
        Args:
            content: æ¶ˆæ¯å†…å®¹
            
        Returns:
            äº¤äº’ç±»å‹
        """
        content_lower = content.lower()
        
        # èµ„é‡‘ç›¸å…³
        if any(word in content_lower for word in ['é’±', 'æ¬¾', 'è½¬è´¦', 'æ”¯ä»˜', 'è´¹ç”¨', 'å›æ‰£', 'ä½£é‡‘']):
            return 'èµ„é‡‘'
        
        # æƒåŠ›ç›¸å…³
        if any(word in content_lower for word in ['å®¡æ‰¹', 'é€šè¿‡', 'æ‰¹å‡†', 'åŒæ„', 'ç…§é¡¾', 'ç‰¹æ®Š']):
            return 'æƒåŠ›'
        
        # ç§˜å¯†ç›¸å…³
        if any(word in content_lower for word in ['ä¿å¯†', 'ç§˜å¯†', 'ç§ä¸‹', 'åˆ«è¯´', 'åˆ é™¤']):
            return 'ç§˜å¯†'
        
        # ä¼šé¢ç›¸å…³
        if any(word in content_lower for word in ['è§é¢', 'åƒé¥­', 'å–èŒ¶', 'åœ°æ–¹', 'è€åœ°æ–¹']):
            return 'ä¼šé¢'
        
        return 'æ™®é€š'
    
    def _is_suspicious_interaction(self, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¯ç–‘äº¤äº’
        
        Args:
            content: æ¶ˆæ¯å†…å®¹
            
        Returns:
            æ˜¯å¦å¯ç–‘
        """
        suspicious_keywords = [
            'å›æ‰£', 'è´¿èµ‚', 'ä½£é‡‘', 'å¥½å¤„è´¹',
            'ä¿å¯†', 'åˆ«è¯´', 'åˆ é™¤è®°å½•',
            'ç‰¹æ®Šç…§é¡¾', 'è¿è§„', 'æš—ç®±æ“ä½œ',
            'è€åœ°æ–¹', 'ç§ä¸‹', 'ç§˜å¯†'
        ]
        
        content_lower = content.lower()
        return any(keyword in content_lower for keyword in suspicious_keywords)
    
    def calculate_centrality(self) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡
        
        Returns:
            ä¸­å¿ƒæ€§æŒ‡æ ‡å­—å…¸
        """
        if self.graph is None or self.graph.number_of_nodes() == 0:
            return {}
        
        centrality_metrics = {}
        
        # åº¦ä¸­å¿ƒæ€§
        degree_centrality = nx.degree_centrality(self.graph)
        
        # æ¥è¿‘ä¸­å¿ƒæ€§ (å¯¹äºä¸è¿é€šå›¾ï¼Œåªè®¡ç®—è¿é€šåˆ†é‡)
        try:
            closeness_centrality = nx.closeness_centrality(self.graph)
        except:
            # å¦‚æœå›¾ä¸è¿é€šï¼Œä½¿ç”¨harmonicä¸­å¿ƒæ€§ä»£æ›¿
            closeness_centrality = nx.harmonic_centrality(self.graph)
        
        # ä¸­ä»‹ä¸­å¿ƒæ€§
        betweenness_centrality = nx.betweenness_centrality(self.graph)
        
        # ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
        try:
            eigenvector_centrality = nx.eigenvector_centrality(self.graph, max_iter=1000)
        except:
            eigenvector_centrality = {node: 0.0 for node in self.graph.nodes()}
        
        # PageRank
        pagerank = nx.pagerank(self.graph)
        
        # ç»„åˆç»“æœ
        for node in self.graph.nodes():
            centrality_metrics[node] = {
                'degree': degree_centrality.get(node, 0.0),
                'closeness': closeness_centrality.get(node, 0.0),
                'betweenness': betweenness_centrality.get(node, 0.0),
                'eigenvector': eigenvector_centrality.get(node, 0.0),
                'pagerank': pagerank.get(node, 0.0)
            }
        
        return centrality_metrics
    
    def detect_communities(self) -> Dict[str, Any]:
        """æ£€æµ‹ç¤¾åŒºï¼ˆå›¢ä¼™ï¼‰
        
        Returns:
            ç¤¾åŒºæ£€æµ‹ç»“æœ
        """
        if self.graph is None or self.graph.number_of_nodes() == 0:
            return {'communities': [], 'modularity': 0.0}
        
        # ä½¿ç”¨Louvainç®—æ³•æ£€æµ‹ç¤¾åŒº
        partition = community_louvain.best_partition(self.graph)
        
        # æŒ‰ç¤¾åŒºåˆ†ç»„
        communities_dict = defaultdict(list)
        for node, community_id in partition.items():
            communities_dict[community_id].append(node)
        
        # è®¡ç®—æ¯ä¸ªç¤¾åŒºçš„æŒ‡æ ‡
        communities = []
        for community_id, members in communities_dict.items():
            # åˆ›å»ºå­å›¾
            subgraph = self.graph.subgraph(members)
            
            # è®¡ç®—å¯†åº¦
            density = nx.density(subgraph)
            
            # è®¡ç®—å†…éƒ¨è¾¹æ•°
            internal_edges = subgraph.number_of_edges()
            
            # è®¡ç®—å¤–éƒ¨è¾¹æ•°
            external_edges = 0
            for node in members:
                for neighbor in self.graph.neighbors(node):
                    if neighbor not in members:
                        external_edges += 1
            external_edges = external_edges // 2  # æ¯æ¡è¾¹è¢«è®¡ç®—ä¸¤æ¬¡
            
            # è®¡ç®—é£é™©åˆ†æ•°ï¼ˆåŸºäºå¯ç–‘äº¤äº’æ¯”ä¾‹ï¼‰
            risk_score = 0.0
            total_suspicious = 0
            total_edges = 0
            for u, v, data in subgraph.edges(data=True):
                total_edges += 1
                total_suspicious += data.get('suspicious_count', 0)
            
            if total_edges > 0:
                risk_score = (total_suspicious / total_edges) * 10
            
            communities.append({
                'id': community_id,
                'members': members,
                'size': len(members),
                'density': round(density, 3),
                'internal_edges': internal_edges,
                'external_edges': external_edges,
                'risk_score': round(min(risk_score, 10.0), 2)
            })
        
        # è®¡ç®—æ¨¡å—åº¦
        modularity = community_louvain.modularity(partition, self.graph)
        
        # æŒ‰é£é™©åˆ†æ•°æ’åº
        communities.sort(key=lambda x: x['risk_score'], reverse=True)
        
        return {
            'communities': communities,
            'modularity': round(modularity, 3),
            'num_communities': len(communities)
        }
    
    def identify_key_players(self, centrality: Dict[str, Dict[str, float]], 
                            communities: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«å…³é”®äººç‰©
        
        Args:
            centrality: ä¸­å¿ƒæ€§æŒ‡æ ‡
            communities: ç¤¾åŒºæ£€æµ‹ç»“æœ
            
        Returns:
            å…³é”®äººç‰©åˆ—è¡¨
        """
        key_players = []
        
        for node in self.graph.nodes():
            metrics = centrality.get(node, {})
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            score = (
                metrics.get('pagerank', 0.0) * 3.0 +
                metrics.get('betweenness', 0.0) * 2.0 +
                metrics.get('degree', 0.0) * 1.5 +
                metrics.get('eigenvector', 0.0) * 1.0
            ) * 10
            
            # ç¡®å®šè§’è‰²
            role = self._determine_role(metrics, node, communities)
            
            key_players.append({
                'name': node,
                'score': round(score, 2),
                'role': role,
                'metrics': {
                    'pagerank': round(metrics.get('pagerank', 0.0), 3),
                    'betweenness': round(metrics.get('betweenness', 0.0), 3),
                    'degree': round(metrics.get('degree', 0.0), 3),
                    'eigenvector': round(metrics.get('eigenvector', 0.0), 3)
                }
            })
        
        # æŒ‰å¾—åˆ†æ’åº
        key_players.sort(key=lambda x: x['score'], reverse=True)
        
        return key_players[:10]  # è¿”å›å‰10å
    
    def _determine_role(self, metrics: Dict[str, float], 
                       node: str, communities: Dict[str, Any]) -> str:
        """ç¡®å®šäººç‰©è§’è‰²
        
        Args:
            metrics: ä¸­å¿ƒæ€§æŒ‡æ ‡
            node: èŠ‚ç‚¹åç§°
            communities: ç¤¾åŒºæ£€æµ‹ç»“æœ
            
        Returns:
            è§’è‰²æè¿°
        """
        betweenness = metrics.get('betweenness', 0.0)
        pagerank = metrics.get('pagerank', 0.0)
        degree = metrics.get('degree', 0.0)
        
        # ä¸­é—´äºº - é«˜ä¸­ä»‹ä¸­å¿ƒæ€§
        if betweenness > 0.3:
            return 'å…³é”®ä¸­é—´äºº'
        
        # æ ¸å¿ƒäººç‰© - é«˜PageRankå’Œé«˜åº¦ä¸­å¿ƒæ€§
        if pagerank > 0.15 and degree > 0.4:
            return 'æ ¸å¿ƒäººç‰©'
        
        # è¿æ¥è€… - é«˜åº¦ä¸­å¿ƒæ€§
        if degree > 0.5:
            return 'æ´»è·ƒè¿æ¥è€…'
        
        # å½±å“è€… - é«˜ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
        if metrics.get('eigenvector', 0.0) > 0.3:
            return 'å½±å“åŠ›äººç‰©'
        
        return 'æ™®é€šå‚ä¸è€…'
    
    def analyze_network_metrics(self) -> Dict[str, Any]:
        """åˆ†æç½‘ç»œæ•´ä½“æŒ‡æ ‡
        
        Returns:
            ç½‘ç»œæŒ‡æ ‡å­—å…¸
        """
        if self.graph is None or self.graph.number_of_nodes() == 0:
            return {}
        
        # åŸºæœ¬æŒ‡æ ‡
        num_nodes = self.graph.number_of_nodes()
        num_edges = self.graph.number_of_edges()
        density = nx.density(self.graph)
        
        # è¿é€šæ€§
        is_connected = nx.is_connected(self.graph)
        if is_connected:
            avg_path_length = nx.average_shortest_path_length(self.graph)
            diameter = nx.diameter(self.graph)
        else:
            # å¯¹äºä¸è¿é€šå›¾ï¼Œè®¡ç®—æœ€å¤§è¿é€šåˆ†é‡
            largest_cc = max(nx.connected_components(self.graph), key=len)
            largest_subgraph = self.graph.subgraph(largest_cc)
            avg_path_length = nx.average_shortest_path_length(largest_subgraph)
            diameter = nx.diameter(largest_subgraph)
        
        # èšç±»ç³»æ•°
        avg_clustering = nx.average_clustering(self.graph)
        
        # åº¦åˆ†å¸ƒ
        degrees = [d for n, d in self.graph.degree()]
        avg_degree = np.mean(degrees) if degrees else 0
        
        return {
            'num_nodes': num_nodes,
            'num_edges': num_edges,
            'density': round(density, 3),
            'is_connected': is_connected,
            'avg_path_length': round(avg_path_length, 2),
            'diameter': diameter,
            'avg_clustering': round(avg_clustering, 3),
            'avg_degree': round(avg_degree, 2),
            'max_degree': max(degrees) if degrees else 0,
            'min_degree': min(degrees) if degrees else 0
        }
    
    def find_bridging_ties(self) -> List[Dict[str, Any]]:
        """å‘ç°æ¡¥æ¢è¿æ¥ï¼ˆè·¨ç¤¾åŒºçš„å…³é”®è¿æ¥ï¼‰
        
        Returns:
            æ¡¥æ¢è¿æ¥åˆ—è¡¨
        """
        if self.graph is None:
            return []
        
        # æ£€æµ‹ç¤¾åŒº
        communities_result = self.detect_communities()
        partition = community_louvain.best_partition(self.graph)
        
        # æ‰¾å‡ºè·¨ç¤¾åŒºçš„è¾¹
        bridging_edges = []
        for u, v, data in self.graph.edges(data=True):
            if partition.get(u) != partition.get(v):
                bridging_edges.append({
                    'person1': u,
                    'person2': v,
                    'community1': partition.get(u),
                    'community2': partition.get(v),
                    'weight': data.get('weight', 0),
                    'suspicious_ratio': data.get('suspicious_ratio', 0.0)
                })
        
        # æŒ‰æƒé‡æ’åº
        bridging_edges.sort(key=lambda x: x['weight'], reverse=True)
        
        return bridging_edges[:10]  # è¿”å›å‰10ä¸ª
    
    def analyze_temporal_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´æ¨¡å¼
        
        Returns:
            æ—¶é—´æ¨¡å¼åˆ†æç»“æœ
        """
        if not self.messages:
            return {}
        
        # æŒ‰å°æ—¶ç»Ÿè®¡
        hour_counts = defaultdict(int)
        # æŒ‰æ˜ŸæœŸç»Ÿè®¡
        weekday_counts = defaultdict(int)
        # æŒ‰æ˜¯å¦å·¥ä½œæ—¶é—´ç»Ÿè®¡
        work_hours = 0
        non_work_hours = 0
        
        for msg in self.messages:
            timestamp = msg.get('timestamp', '')
            if not timestamp:
                continue
            
            try:
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                hour = dt.hour
                weekday = dt.weekday()
                
                hour_counts[hour] += 1
                weekday_counts[weekday] += 1
                
                # åˆ¤æ–­æ˜¯å¦å·¥ä½œæ—¶é—´ (9-18ç‚¹)
                if 9 <= hour <= 18:
                    work_hours += 1
                else:
                    non_work_hours += 1
            except:
                continue
        
        # æ‰¾å‡ºæœ€æ´»è·ƒçš„æ—¶é—´æ®µ
        peak_hour = max(hour_counts.items(), key=lambda x: x[1])[0] if hour_counts else 0
        
        return {
            'hour_distribution': dict(hour_counts),
            'weekday_distribution': dict(weekday_counts),
            'peak_hour': peak_hour,
            'work_hours_ratio': round(work_hours / max(work_hours + non_work_hours, 1), 3),
            'non_work_hours_ratio': round(non_work_hours / max(work_hours + non_work_hours, 1), 3)
        }
    
    def generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦
        
        Returns:
            åˆ†ææ‘˜è¦
        """
        if self.graph is None or self.graph.number_of_nodes() == 0:
            return {'error': 'æ²¡æœ‰å¯åˆ†æçš„ç½‘ç»œæ•°æ®'}
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        network_metrics = self.analyze_network_metrics()
        centrality = self.calculate_centrality()
        communities = self.detect_communities()
        key_players = self.identify_key_players(centrality, communities)
        bridging_ties = self.find_bridging_ties()
        temporal_patterns = self.analyze_temporal_patterns()
        
        # è¯„ä¼°æ•´ä½“é£é™©
        high_risk_communities = [c for c in communities['communities'] if c['risk_score'] >= 6.0]
        overall_risk = 'ä½'
        if len(high_risk_communities) >= 2:
            overall_risk = 'é«˜'
        elif len(high_risk_communities) >= 1:
            overall_risk = 'ä¸­'
        
        return {
            'overall_risk': overall_risk,
            'network_metrics': network_metrics,
            'centrality': centrality,
            'communities': communities,
            'key_players': key_players,
            'bridging_ties': bridging_ties,
            'temporal_patterns': temporal_patterns,
            'summary': {
                'total_participants': network_metrics.get('num_nodes', 0),
                'total_connections': network_metrics.get('num_edges', 0),
                'num_communities': communities.get('num_communities', 0),
                'high_risk_communities': len(high_risk_communities),
                'network_density': network_metrics.get('density', 0.0),
                'clustering_coefficient': network_metrics.get('avg_clustering', 0.0)
            }
        }
    
    def save_report(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜åˆ†ææŠ¥å‘Š
        
        Args:
            results: åˆ†æç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    def visualize_network(self, output_path: str):
        """å¯è§†åŒ–å…³ç³»ç½‘ç»œï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            output_path: è¾“å‡ºHTMLæ–‡ä»¶è·¯å¾„
        """
        if self.graph is None or self.graph.number_of_nodes() == 0:
            print("âŒ æ²¡æœ‰å¯å¯è§†åŒ–çš„ç½‘ç»œæ•°æ®")
            return
        
        try:
            import plotly.graph_objects as go
            
            # è·å–å¸ƒå±€
            pos = nx.spring_layout(self.graph, k=2, iterations=50)
            
            # å‡†å¤‡èŠ‚ç‚¹æ•°æ®
            node_x = []
            node_y = []
            node_text = []
            node_sizes = []
            
            for node in self.graph.nodes():
                x, y = pos[node]
                node_x.append(x)
                node_y.append(y)
                node_text.append(node)
                # æ ¹æ®åº¦æ•°è°ƒæ•´å¤§å°
                node_sizes.append(self.graph.degree(node) * 10 + 20)
            
            # å‡†å¤‡è¾¹æ•°æ®
            edge_x = []
            edge_y = []
            
            for edge in self.graph.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                edge_x.extend([x0, x1, None])
                edge_y.extend([y0, y1, None])
            
            # åˆ›å»ºå›¾
            fig = go.Figure()
            
            # æ·»åŠ è¾¹
            fig.add_trace(go.Scatter(
                x=edge_x, y=edge_y,
                line=dict(width=0.5, color='#888'),
                hoverinfo='none',
                mode='lines'
            ))
            
            # æ·»åŠ èŠ‚ç‚¹
            fig.add_trace(go.Scatter(
                x=node_x, y=node_y,
                mode='markers+text',
                hoverinfo='text',
                text=node_text,
                textposition='top center',
                marker=dict(
                    size=node_sizes,
                    color='lightblue',
                    line=dict(width=2, color='DarkBlue')
                )
            ))
            
            fig.update_layout(
                title='åè…è´¥å…³ç³»ç½‘ç»œå›¾',
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20, l=5, r=5, t=40),
                annotations=[
                    dict(
                        text="å…³ç³»ç½‘ç»œå¯è§†åŒ–",
                        showarrow=False,
                        xref="paper", yref="paper",
                        x=0.005, y=-0.002,
                        xanchor='left', yanchor='bottom',
                        font=dict(size=12)
                    )
                ]
            )
            
            # ä¿å­˜HTML
            fig.write_html(output_path)
            print(f"âœ… ç½‘ç»œå¯è§†åŒ–å·²ä¿å­˜: {output_path}")
            
        except ImportError:
            print("âŒ éœ€è¦å®‰è£… plotly: pip install plotly")
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python relationship_analyzer.py <input_file> <output_file> [--visualize]")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2]
    visualize = '--visualize' in sys.argv
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = RelationshipAnalyzer()
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½èŠå¤©æ•°æ®...")
    messages = analyzer.load_messages(input_file)
    
    if not messages:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°æ¶ˆæ¯æ•°æ®")
        sys.exit(1)
    
    # æ„å»ºç½‘ç»œ
    print("ğŸ•¸ï¸ æ„å»ºå…³ç³»ç½‘ç»œ...")
    analyzer.build_network(messages)
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    print("ğŸ“ˆ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    results = analyzer.generate_summary()
    
    # ä¿å­˜æŠ¥å‘Š
    analyzer.save_report(results, output_file)
    
    # å¯è§†åŒ–
    if visualize:
        print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–...")
        viz_path = output_file.replace('.json', '_network.html')
        analyzer.visualize_network(viz_path)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ“Š åˆ†ææ‘˜è¦")
    print("="*50)
    summary = results.get('summary', {})
    print(f"å‚ä¸äººæ•°: {summary.get('total_participants', 0)}")
    print(f"è¿æ¥æ•°é‡: {summary.get('total_connections', 0)}")
    print(f"ç¤¾åŒºæ•°é‡: {summary.get('num_communities', 0)}")
    print(f"é«˜é£é™©ç¤¾åŒº: {summary.get('high_risk_communities', 0)}")
    print(f"ç½‘ç»œå¯†åº¦: {summary.get('network_density', 0.0):.3f}")
    print(f"èšç±»ç³»æ•°: {summary.get('clustering_coefficient', 0.0):.3f}")
    print(f"æ•´ä½“é£é™©: {results.get('overall_risk', 'æœªçŸ¥')}")
    
    # æ‰“å°å…³é”®äººç‰©
    print("\nğŸ¯ å…³é”®äººç‰© (Top 5):")
    for i, player in enumerate(results.get('key_players', [])[:5], 1):
        print(f"{i}. {player['name']} - {player['role']} (å¾—åˆ†: {player['score']})")
    
    print("\nâœ… åˆ†æå®Œæˆ!")


if __name__ == '__main__':
    main()
