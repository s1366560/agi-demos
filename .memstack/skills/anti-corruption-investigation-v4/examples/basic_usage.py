#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ - åè…è´¥è°ƒæŸ¥æŠ€èƒ½ v4.0
Basic Usage Example for Anti-Corruption Investigation Skill v4.0
"""

import sys
import os

# æ·»åŠ scriptsç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'scripts'))

from relationship_analyzer import RelationshipAnalyzer
from scalable_analyzer import ScalableAnalyzer


def example_1_message_analysis():
    """ç¤ºä¾‹1: æ¶ˆæ¯åˆ†æ"""
    print("="*60)
    print("ç¤ºä¾‹1: æ¶ˆæ¯åˆ†æ")
    print("="*60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ScalableAnalyzer(batch_size=1000)
    
    # åˆ†æå•æ¡æ¶ˆæ¯
    test_message = {
        'timestamp': '2024-01-15T14:30:00',
        'sender': 'å¼ ä¸‰',
        'receiver': 'æå››',
        'content': 'é‚£ç¬”é’±å‡†å¤‡å¥½äº†å—ï¼Ÿè€åœ°æ–¹è§ã€‚'
    }
    
    result = analyzer.analyze_message(test_message)
    
    print(f"\næ¶ˆæ¯å†…å®¹: {test_message['content']}")
    print(f"æ˜¯å¦å¯ç–‘: {'æ˜¯' if result['is_suspicious'] else 'å¦'}")
    print(f"é£é™©ç­‰çº§: {result['risk_level']}")
    print(f"è¯­ä¹‰é£é™©: {result['semantic_risk']:.2f}")
    print(f"æ£€æµ‹åˆ°çš„æ¨¡å¼: {[p['category'] for p in result['patterns']]}")
    print(f"è¡Œä¸ºå¼‚å¸¸: {result['behavioral_flags']}")


def example_2_relationship_analysis():
    """ç¤ºä¾‹2: å…³ç³»ç½‘ç»œåˆ†æ"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹2: å…³ç³»ç½‘ç»œåˆ†æ")
    print("="*60)
    
    # åˆ›å»ºå…³ç³»åˆ†æå™¨
    analyzer = RelationshipAnalyzer()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    data_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'relationship_test_data.jsonl')
    
    if not os.path.exists(data_path):
        print(f"âš ï¸ æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {data_path}")
        print("è¯·å…ˆè¿è¡Œ: python scripts/generate_test_data.py")
        return
    
    # åŠ è½½æ¶ˆæ¯
    messages = analyzer.load_messages(data_path)
    
    # æ„å»ºç½‘ç»œ
    analyzer.build_network(messages)
    
    # è®¡ç®—ä¸­å¿ƒæ€§
    print("\nğŸ“Š ä¸­å¿ƒæ€§æŒ‡æ ‡:")
    centrality = analyzer.calculate_centrality()
    for person, metrics in list(centrality.items())[:5]:
        print(f"\n{person}:")
        print(f"  åº¦ä¸­å¿ƒæ€§: {metrics['degree']:.3f}")
        print(f"  ä¸­ä»‹ä¸­å¿ƒæ€§: {metrics['betweenness']:.3f}")
        print(f"  PageRank: {metrics['pagerank']:.3f}")
    
    # æ£€æµ‹ç¤¾åŒº
    print("\nğŸ‘¥ ç¤¾åŒºæ£€æµ‹:")
    communities = analyzer.detect_communities()
    for i, community in enumerate(communities['communities'][:3], 1):
        print(f"\nç¤¾åŒº {i}:")
        print(f"  æˆå‘˜: {', '.join(community['members'])}")
        print(f"  å¯†åº¦: {community['density']:.3f}")
        print(f"  é£é™©åˆ†æ•°: {community['risk_score']}/10")
    
    # è¯†åˆ«å…³é”®äººç‰©
    print("\nğŸ¯ å…³é”®äººç‰©:")
    key_players = analyzer.identify_key_players(centrality, communities)
    for i, player in enumerate(key_players[:5], 1):
        print(f"\n{i}. {player['name']} - {player['role']}")
        print(f"   å¾—åˆ†: {player['score']}")
        print(f"   PageRank: {player['metrics']['pagerank']:.3f}")


def example_3_large_scale_analysis():
    """ç¤ºä¾‹3: å¤§è§„æ¨¡æ•°æ®åˆ†æ"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹3: å¤§è§„æ¨¡æ•°æ®åˆ†æ")
    print("="*60)
    
    # åˆ›å»ºå¯æ‰©å±•åˆ†æå™¨
    analyzer = ScalableAnalyzer(batch_size=10000, workers=8)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    data_path = '/tmp/large_test.jsonl'
    print(f"\nç”Ÿæˆæµ‹è¯•æ•°æ®åˆ°: {data_path}")
    
    from scripts.generate_test_data import generate_large_dataset
    generate_large_dataset(data_path, num_messages=10000)
    
    # åˆ†ææ•°æ®
    output_path = '/tmp/analysis_report.json'
    results = analyzer.analyze_large_dataset(data_path, output_path)
    
    print(f"\nâœ… åˆ†æå®Œæˆ!")
    print(f"é£é™©ç­‰çº§: {results['overall_risk']}")
    print(f"é£é™©åˆ†æ•°: {results['risk_score']}/10")


def example_4_complete_workflow():
    """ç¤ºä¾‹4: å®Œæ•´å·¥ä½œæµç¨‹"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹4: å®Œæ•´å·¥ä½œæµç¨‹")
    print("="*60)
    
    # æ­¥éª¤1: æ¶ˆæ¯åˆ†æ
    print("\næ­¥éª¤1: æ¶ˆæ¯åˆ†æ")
    scalable_analyzer = ScalableAnalyzer()
    
    data_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'relationship_test_data.jsonl')
    if not os.path.exists(data_path):
        print("âš ï¸ æµ‹è¯•æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
        return
    
    output_path = '/tmp/message_analysis.json'
    message_results = scalable_analyzer.analyze_large_dataset(data_path, output_path)
    
    # æ­¥éª¤2: å…³ç³»ç½‘ç»œåˆ†æ
    print("\næ­¥éª¤2: å…³ç³»ç½‘ç»œåˆ†æ")
    relationship_analyzer = RelationshipAnalyzer()
    messages = relationship_analyzer.load_messages(data_path)
    relationship_analyzer.build_network(messages)
    
    # æ­¥éª¤3: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\næ­¥éª¤3: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
    summary = relationship_analyzer.generate_summary()
    
    # ä¿å­˜å®Œæ•´æŠ¥å‘Š
    report_path = '/tmp/complete_report.json'
    relationship_analyzer.save_report(summary, report_path)
    
    # æ­¥éª¤4: å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    print("\næ­¥éª¤4: ç”Ÿæˆå¯è§†åŒ–")
    try:
        viz_path = '/tmp/network_visualization.html'
        relationship_analyzer.visualize_network(viz_path)
        print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜: {viz_path}")
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    print("\nâœ… å®Œæ•´å·¥ä½œæµç¨‹å®Œæˆ!")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ åè…è´¥è°ƒæŸ¥æŠ€èƒ½ v4.0 - åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹")
    print("="*60)
    
    # è¿è¡Œç¤ºä¾‹
    example_1_message_analysis()
    example_2_relationship_analysis()
    example_3_large_scale_analysis()
    example_4_complete_workflow()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print("="*60)


if __name__ == '__main__':
    main()
