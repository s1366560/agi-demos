#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯æ–‡ä»¶ä¸Šä¼ å®Œæ•´æ€§æµ‹è¯•

ç›´æ¥æµ‹è¯•ï¼š
1. ä¸Šä¼ æ–‡ä»¶åˆ° S3
2. ä» S3 ä¸‹è½½
3. éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
4. æµ‹è¯• prepare_for_sandbox çš„ base64 ç¼–ç 
5. æ¨¡æ‹Ÿ import_file çš„è§£ç å’Œå†™å…¥
"""

import asyncio
import base64
import hashlib
import os
import sys
import tempfile

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.configuration.config import get_settings


def create_test_xlsx_data() -> bytes:
    """åˆ›å»ºæ¨¡æ‹Ÿçš„ Excel æ–‡ä»¶æ•°æ®ï¼ˆå®é™…æ˜¯ä¸€ä¸ªå°å‹ ZIP ç»“æ„ï¼‰"""
    # çœŸå®çš„ xlsx æ–‡ä»¶æ˜¯ä¸€ä¸ª ZIP æ–‡ä»¶
    # è¿™é‡Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„äºŒè¿›åˆ¶æ•°æ®ç”¨äºæµ‹è¯•
    import io
    import zipfile

    buffer = io.BytesIO()
    with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        # æ·»åŠ ä¸€äº›æµ‹è¯•å†…å®¹
        zf.writestr("[Content_Types].xml", '<?xml version="1.0"?><Types></Types>')
        zf.writestr("_rels/.rels", '<?xml version="1.0"?><Relationships></Relationships>')
        # æ·»åŠ ä¸€äº›éšæœºæ•°æ®æ¨¡æ‹ŸçœŸå® Excel å†…å®¹
        zf.writestr(
            "xl/worksheets/sheet1.xml",
            f'<?xml version="1.0"?><worksheet><data>{os.urandom(10000).hex()}</data></worksheet>',
        )

    return buffer.getvalue()


async def test_s3_upload_download():
    """æµ‹è¯• S3 ä¸Šä¼ å’Œä¸‹è½½çš„å®Œæ•´æ€§"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: S3 ä¸Šä¼ /ä¸‹è½½å®Œæ•´æ€§")
    print("=" * 60)

    from src.infrastructure.adapters.secondary.storage.s3_storage_adapter import S3StorageAdapter

    settings = get_settings()

    # åˆ›å»º S3 adapter
    storage = S3StorageAdapter(
        endpoint_url=settings.s3_endpoint_url,
        access_key_id=settings.aws_access_key_id,
        secret_access_key=settings.aws_secret_access_key,
        bucket_name=settings.s3_bucket_name,
        region=settings.aws_region,
    )

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_xlsx_data()
    original_hash = hashlib.md5(test_data).hexdigest()

    print(f"åŸå§‹æ•°æ®å¤§å°: {len(test_data)} bytes")
    print(f"åŸå§‹æ•°æ® MD5: {original_hash}")
    print(f"åŸå§‹æ•°æ®å‰32å­—èŠ‚(hex): {test_data[:32].hex()}")

    # æµ‹è¯•å¯¹è±¡é”®
    test_key = f"test/integrity_test_{os.urandom(4).hex()}.xlsx"

    try:
        # ä¸Šä¼ 
        print(f"\næ­£åœ¨ä¸Šä¼ åˆ° S3: {test_key}")
        await storage.upload_file(
            file_content=test_data,
            object_key=test_key,
            content_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            metadata={"filename": "æµ‹è¯•æ–‡ä»¶.xlsx", "purpose": "both"},
        )
        print("âœ… ä¸Šä¼ æˆåŠŸ")

        # ä¸‹è½½
        print(f"æ­£åœ¨ä» S3 ä¸‹è½½: {test_key}")
        downloaded_data = await storage.get_file(test_key)

        if downloaded_data is None:
            print("âŒ ä¸‹è½½å¤±è´¥ï¼šæ–‡ä»¶ä¸å­˜åœ¨")
            return False

        downloaded_hash = hashlib.md5(downloaded_data).hexdigest()

        print(f"ä¸‹è½½æ•°æ®å¤§å°: {len(downloaded_data)} bytes")
        print(f"ä¸‹è½½æ•°æ® MD5: {downloaded_hash}")
        print(f"ä¸‹è½½æ•°æ®å‰32å­—èŠ‚(hex): {downloaded_data[:32].hex()}")

        # éªŒè¯
        if original_hash == downloaded_hash:
            print("âœ… S3 ä¸Šä¼ /ä¸‹è½½å®Œæ•´æ€§éªŒè¯é€šè¿‡ï¼")

            # æ¸…ç†
            await storage.delete_file(test_key)
            return True
        else:
            print("âŒ S3 ä¸Šä¼ /ä¸‹è½½æ•°æ®ä¸ä¸€è‡´ï¼")
            print(f"  æœŸæœ›: {original_hash}")
            print(f"  å®é™…: {downloaded_hash}")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_full_sandbox_pipeline():
    """æµ‹è¯•å®Œæ•´çš„ sandbox å¯¼å…¥ç®¡é“"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: å®Œæ•´ Sandbox å¯¼å…¥ç®¡é“")
    print("=" * 60)

    from src.infrastructure.adapters.secondary.storage.s3_storage_adapter import S3StorageAdapter

    settings = get_settings()

    # åˆ›å»º S3 adapter
    storage = S3StorageAdapter(
        endpoint_url=settings.s3_endpoint_url,
        access_key_id=settings.aws_access_key_id,
        secret_access_key=settings.aws_secret_access_key,
        bucket_name=settings.s3_bucket_name,
        region=settings.aws_region,
    )

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_xlsx_data()
    original_hash = hashlib.md5(test_data).hexdigest()

    print(f"[1] åŸå§‹æ–‡ä»¶: {len(test_data)} bytes, MD5: {original_hash}")

    test_key = f"test/sandbox_test_{os.urandom(4).hex()}.xlsx"

    try:
        # Step 1: ä¸Šä¼ åˆ° S3
        await storage.upload_file(
            file_content=test_data,
            object_key=test_key,
            content_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
        print(f"[2] S3 ä¸Šä¼ æˆåŠŸ: {test_key}")

        # Step 2: ä» S3 ä¸‹è½½ï¼ˆæ¨¡æ‹Ÿ prepare_for_sandboxï¼‰
        content = await storage.get_file(test_key)
        if content is None:
            print("âŒ S3 ä¸‹è½½å¤±è´¥")
            return False

        s3_hash = hashlib.md5(content).hexdigest()
        print(f"[3] S3 ä¸‹è½½: {len(content)} bytes, MD5: {s3_hash}")

        # Step 3: Base64 ç¼–ç ï¼ˆprepare_for_sandboxï¼‰
        content_base64 = base64.b64encode(content).decode("utf-8")
        print(f"[4] Base64 ç¼–ç : {len(content_base64)} chars")

        # Step 4: æ¨¡æ‹Ÿ JSON-RPC ä¼ è¾“
        import json

        message = {
            "jsonrpc": "2.0",
            "method": "tools/call",
            "params": {
                "name": "import_file",
                "arguments": {
                    "filename": "æµ‹è¯•æ–‡ä»¶.xlsx",
                    "content_base64": content_base64,
                },
            },
        }
        json_str = json.dumps(message)
        parsed = json.loads(json_str)
        received_base64 = parsed["params"]["arguments"]["content_base64"]
        print(f"[5] JSON ä¼ è¾“: ç¼–ç å‰={len(content_base64)}, ä¼ è¾“å={len(received_base64)}")

        # Step 5: Base64 è§£ç ï¼ˆimport_fileï¼‰
        decoded = base64.b64decode(received_base64)
        decoded_hash = hashlib.md5(decoded).hexdigest()
        print(f"[6] Base64 è§£ç : {len(decoded)} bytes, MD5: {decoded_hash}")

        # Step 6: å†™å…¥æ–‡ä»¶ï¼ˆimport_fileï¼‰
        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as f:
            temp_path = f.name
            f.write(decoded)

        # Step 7: éªŒè¯æœ€ç»ˆæ–‡ä»¶
        with open(temp_path, "rb") as f:
            final_data = f.read()
        final_hash = hashlib.md5(final_data).hexdigest()
        print(f"[7] æœ€ç»ˆæ–‡ä»¶: {len(final_data)} bytes, MD5: {final_hash}")

        os.unlink(temp_path)

        # æ¸…ç† S3
        await storage.delete_file(test_key)

        # éªŒè¯
        if original_hash == final_hash:
            print("\nâœ… å®Œæ•´ç®¡é“æµ‹è¯•é€šè¿‡ï¼æ–‡ä»¶åœ¨æ•´ä¸ªæµç¨‹ä¸­ä¿æŒå®Œæ•´")
            return True
        else:
            print("\nâŒ æ–‡ä»¶åœ¨ç®¡é“ä¸­æŸåï¼")
            print(f"  åŸå§‹: {original_hash}")
            print(f"  æœ€ç»ˆ: {final_hash}")

            # æ‰¾å‡ºæŸåç‚¹
            if original_hash != s3_hash:
                print("  ğŸ’¥ æŸåç‚¹: S3 ä¸Šä¼ /ä¸‹è½½")
            elif s3_hash != decoded_hash:
                print("  ğŸ’¥ æŸåç‚¹: Base64 ç¼–è§£ç æˆ– JSON ä¼ è¾“")
            elif decoded_hash != final_hash:
                print("  ğŸ’¥ æŸåç‚¹: æ–‡ä»¶å†™å…¥")

            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_real_file_if_exists():
    """å¦‚æœæœ‰çœŸå®çš„ä¸Šä¼ æ–‡ä»¶ï¼Œæµ‹è¯•å…¶å®Œæ•´æ€§"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: æ£€æŸ¥ç°æœ‰ä¸Šä¼ æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰")
    print("=" * 60)

    from src.infrastructure.adapters.secondary.storage.s3_storage_adapter import S3StorageAdapter

    settings = get_settings()

    storage = S3StorageAdapter(
        endpoint_url=settings.s3_endpoint_url,
        access_key_id=settings.aws_access_key_id,
        secret_access_key=settings.aws_secret_access_key,
        bucket_name=settings.s3_bucket_name,
        region=settings.aws_region,
    )

    # åˆ—å‡ºæœ€è¿‘çš„é™„ä»¶
    try:
        files = await storage.list_files("attachments/", max_keys=10)

        if not files:
            print("æ²¡æœ‰æ‰¾åˆ°å·²ä¸Šä¼ çš„é™„ä»¶æ–‡ä»¶")
            return True

        print(f"æ‰¾åˆ° {len(files)} ä¸ªé™„ä»¶æ–‡ä»¶")

        for file_key in files[:3]:  # åªæ£€æŸ¥å‰3ä¸ª
            print(f"\næ£€æŸ¥æ–‡ä»¶: {file_key}")

            content = await storage.get_file(file_key)
            if content:
                print(f"  å¤§å°: {len(content)} bytes")
                print(f"  MD5: {hashlib.md5(content).hexdigest()}")
                print(f"  å‰16å­—èŠ‚(hex): {content[:16].hex()}")

                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ ZIP/Office æ–‡ä»¶
                if content[:4] == b"PK\x03\x04":
                    print("  âœ… æ–‡ä»¶å¤´æ­£ç¡®ï¼ˆZIP/Office æ ¼å¼ï¼‰")
                elif content[:4] == b"%PDF":
                    print("  âœ… æ–‡ä»¶å¤´æ­£ç¡®ï¼ˆPDF æ ¼å¼ï¼‰")
                elif content[:2] == b"\xff\xd8":
                    print("  âœ… æ–‡ä»¶å¤´æ­£ç¡®ï¼ˆJPEG æ ¼å¼ï¼‰")
                elif content[:8] == b"\x89PNG\r\n\x1a\n":
                    print("  âœ… æ–‡ä»¶å¤´æ­£ç¡®ï¼ˆPNG æ ¼å¼ï¼‰")
                else:
                    print(f"  âš ï¸ æœªçŸ¥æ–‡ä»¶æ ¼å¼ï¼Œå‰4å­—èŠ‚: {content[:4]}")

        return True

    except Exception as e:
        print(f"æ£€æŸ¥å¤±è´¥: {e}")
        return False


async def main():
    print("=" * 60)
    print("ç«¯åˆ°ç«¯æ–‡ä»¶ä¸Šä¼ å®Œæ•´æ€§æµ‹è¯•")
    print("=" * 60)

    all_passed = True

    try:
        all_passed &= await test_s3_upload_download()
        all_passed &= await test_full_sandbox_pipeline()
        all_passed &= await test_real_file_if_exists()
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        all_passed = False

    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡ï¼")
        print("\nåç«¯å®Œæ•´é“¾è·¯æ˜¯æ­£ç¡®çš„ã€‚")
        print("å¦‚æœç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ä»ç„¶æŸåï¼Œé—®é¢˜ä¸€å®šåœ¨ï¼š")
        print("")
        print("1. ã€å‰ç«¯ã€‘æµè§ˆå™¨è¯»å–æ–‡ä»¶æ—¶")
        print("2. ã€ç½‘ç»œã€‘HTTP è¯·æ±‚ä¼ è¾“æ—¶")
        print("3. ã€ç‰¹å®šæ–‡ä»¶ã€‘æŸäº›æ–‡ä»¶ç±»å‹æœ‰ç‰¹æ®Šé—®é¢˜")
        print("")
        print("å»ºè®®æ£€æŸ¥ï¼š")
        print("- æµè§ˆå™¨æ§åˆ¶å°æ˜¯å¦æœ‰é”™è¯¯")
        print("- ç½‘ç»œé¢æ¿ä¸­è¯·æ±‚çš„å¤§å°æ˜¯å¦æ­£ç¡®")
        print("- å°è¯•ç”¨ curl ç›´æ¥ä¸Šä¼ æ–‡ä»¶æµ‹è¯•")
    else:
        print("âŒ å­˜åœ¨æµ‹è¯•å¤±è´¥ï¼")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
