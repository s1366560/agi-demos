# ==========================================
# MemStack Backend Configuration
# ==========================================

# --- API Server Settings ---
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
# Comma-separated list of allowed origins for CORS
API_ALLOWED_ORIGINS=*

# --- Security Settings ---
# Generate a secure key for production: openssl rand -hex 32
SECRET_KEY=2a0237a3dbd413a52f04f0c4a0e6b183f16694229e8dca3a9b8eb9e33c7d3419
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
REQUIRE_API_KEY=true
API_KEY_HEADER_NAME=Authorization

# LLM Provider API Key Encryption (32 bytes = 64 hex chars)
# Generate with: python -c "import os; print(os.urandom(32).hex())"
# If not set or invalid, a random development key is used (NOT FOR PRODUCTION)
LLM_ENCRYPTION_KEY=2a0237a3dbd413a52f04f0c4a0e6b183f16694229e8dca3a9b8eb9e33c7d3419

# --- Database Settings ---
# Neo4j Graph Database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password_here

# PostgreSQL for Metadata
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=memstack
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password_here

# PostgreSQL Connection Pool (High Concurrency)
POSTGRES_POOL_SIZE=20
POSTGRES_MAX_OVERFLOW=40
POSTGRES_POOL_RECYCLE=3600
POSTGRES_POOL_PRE_PING=true

# PostgreSQL Read Replica (Optional - for read scaling)
# POSTGRES_READ_REPLICA_HOST=localhost
# POSTGRES_READ_REPLICA_PORT=5434

# Redis for Caching
REDIS_HOST=localhost
REDIS_PORT=6379
# REDIS_PASSWORD=your_redis_password

# Ray (Actor Runtime)
RAY_ADDRESS=ray://ray-head:10001
RAY_NAMESPACE=memstack
# RAY_LOG_TO_DRIVER=false
# Agent runtime mode: auto (prefer Ray, fallback local), ray (Ray only), local (local only)
AGENT_RUNTIME_MODE=auto

# Ray Actor Runtime

# --- LLM Provider Settings (Bootstrap Only) ---
# NOTE: MemStack now stores provider configuration in the database.
# The variables below are ONLY used for the initial bootstrap on first run.
# After the first run, these settings are ignored and must be managed via the Admin API.
#
# To bootstrap a default provider, you can set the following environment variables:
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o
#
# Supported providers: openai, gemini, dashscope, deepseek, zai, anthropic
# See AGENTS.md for full list of supported bootstrap variables.


# --- Embedding Management ---
# Automatically clear old embeddings when switching LLM providers with different dimensions
# Set to 'false' to disable automatic clearing
AUTO_CLEAR_MISMATCHED_EMBEDDINGS=true
# EMBEDDING_DIMENSION=           # Auto-detected from model. Set to override (e.g. 1024, 1536)
EMBEDDING_INDEX_AUTO_CREATE=true # 启动时自动创建向量索引
# --- Web Search & Scraping Settings ---
# Tavily API (Web Search)
# Get key from: https://api.tavily.com/
TAVILY_API_KEY=your_tavily_api_key_here
TAVILY_MAX_RESULTS=10
TAVILY_SEARCH_DEPTH=basic
# TAVILY_INCLUDE_DOMAINS=["wikipedia.org", "github.com"]
# TAVILY_EXCLUDE_DOMAINS=["spam.com"]

# Playwright (Web Scraping)
PLAYWRIGHT_TIMEOUT=30000
PLAYWRIGHT_HEADLESS=true
PLAYWRIGHT_MAX_CONTENT_LENGTH=10000

# Cache TTL for web search results (seconds)
WEB_SEARCH_CACHE_TTL=3600

# --- Graphiti Settings ---
GRAPHITI_SEMAPHORE_LIMIT=10
MAX_ASYNC_WORKERS=20
RUN_BACKGROUND_WORKERS=true
QUEUE_BATCH_SIZE=1

# --- LLM Cache & Timeout Settings ---
LLM_TIMEOUT=300
LLM_STREAM_TIMEOUT=600
LLM_CONCURRENCY_LIMIT=8
LLM_MAX_RETRIES=3
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=3600

# --- Monitoring & Telemetry ---
ENABLE_METRICS=true
METRICS_PORT=9090

# OpenTelemetry (Distributed Tracing & Metrics)
ENABLE_TELEMETRY=true
SERVICE_NAME=memstack
ENVIRONMENT=development

# OTLP Exporter (OpenTelemetry Collector)
# Use gRPC endpoint (recommended): http://localhost:4317
# Use HTTP endpoint: http://localhost:4318
# For production with TLS: https://your-collector.example.com:4317
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
# OTEL_EXPORTER_OTLP_HEADERS=authorization=Bearer your-token
# OTEL_EXPORTER_OTLP_PROTOCOL=grpc

# Tracing Sampling (0.0 to 1.0, where 1.0 = 100% sampling)
OTEL_TRACES_SAMPLER=traceidratio
OTEL_TRACES_SAMPLER_ARG=1.0

# Batch export timeout (milliseconds)
OTEL_BATCH_TIMEOUT=30000

# Resource attributes (optional)
# OTEL_RESOURCE_ATTRIBUTES=service.version=1.0.0,deployment.environment=production

# Langfuse LLM Observability (Self-hosted V3)
# Enable/disable Langfuse tracing for all LLM calls
LANGFUSE_ENABLED=false
# Get these from Langfuse UI: http://localhost:3001 -> Settings -> API Keys
# LANGFUSE_PUBLIC_KEY=pk-lf-xxx
# LANGFUSE_SECRET_KEY=sk-lf-xxx
# Self-hosted Langfuse URL (see docker-compose.yml for setup)
LANGFUSE_HOST=http://localhost:3001
# Sampling rate: 1.0 = trace all requests, 0.1 = 10% sampling
LANGFUSE_SAMPLE_RATE=1.0

# Langfuse Docker Compose Settings (for self-hosted deployment)
# Security keys - CHANGE THESE IN PRODUCTION!
# Generate with: openssl rand -hex 32
# LANGFUSE_NEXTAUTH_SECRET=memstack-langfuse-secret-change-in-production
# LANGFUSE_SALT=memstack-langfuse-salt-change-in-production
# LANGFUSE_ENCRYPTION_KEY=0000000000000000000000000000000000000000000000000000000000000000

# --- Logging ---
LOG_LEVEL=INFO
# Options: json, text
LOG_FORMAT=json

# --- S3/MinIO Storage Settings ---
# For AWS S3
# AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=your_aws_access_key_here
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key_here
# S3_BUCKET_NAME=memstack-files

# For local MinIO (development)
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
S3_BUCKET_NAME=memstack-files
S3_ENDPOINT_URL=http://localhost:9000
# Bypass system proxy for S3 requests (useful when local proxy interferes with MinIO)
S3_NO_PROXY=true
PRESIGNED_URL_EXPIRATION=3600

# --- Sandbox Code Execution Settings ---
SANDBOX_DEFAULT_PROVIDER=docker
SANDBOX_DEFAULT_IMAGE=sandbox-mcp-server:latest
SANDBOX_TIMEOUT_SECONDS=300
SANDBOX_MEMORY_LIMIT=2G
SANDBOX_CPU_LIMIT=2
SANDBOX_NETWORK_ISOLATED=true

# --- Agent Skill System (L2 Layer) Settings ---
# Threshold for skill prompt injection (0.5 = medium match score)
AGENT_SKILL_MATCH_THRESHOLD=0.5
# Threshold for direct skill execution (0.8 = high confidence match)
AGENT_SKILL_DIRECT_EXECUTE_THRESHOLD=0.8
# Whether to fallback to LLM when skill execution fails
AGENT_SKILL_FALLBACK_ON_ERROR=true
# Timeout for skill direct execution in seconds (increased from 60 to 300 for long-running tasks)
AGENT_SKILL_EXECUTION_TIMEOUT=300

# --- Context Compression Settings ---
# Adaptive compression trigger thresholds (0.0-1.0, must be ordered l1 < l2 < l3)
# COMPRESSION_L1_TRIGGER_PCT=0.60
# COMPRESSION_L2_TRIGGER_PCT=0.80
# COMPRESSION_L3_TRIGGER_PCT=0.90
# Messages per summary chunk for L2 incremental summarization
# COMPRESSION_CHUNK_SIZE=10
# Max tokens for generated summaries
# COMPRESSION_SUMMARY_MAX_TOKENS=500
# L1: Minimum recoverable tokens before pruning is triggered
# COMPRESSION_PRUNE_MIN_TOKENS=20000
# L1: Protect recent N tokens of tool outputs from pruning
# COMPRESSION_PRUNE_PROTECT_TOKENS=40000
# L1: Comma-separated tool names whose output is never pruned
# COMPRESSION_PRUNE_PROTECTED_TOOLS=skill
# L1: Truncate assistant messages longer than this (chars)
# COMPRESSION_ASSISTANT_TRUNCATE_CHARS=2000
# Role-aware summary truncation limits (chars per role)
# COMPRESSION_TRUNCATE_USER=800
# COMPRESSION_TRUNCATE_ASSISTANT=300
# COMPRESSION_TRUNCATE_TOOL=200
# COMPRESSION_TRUNCATE_SYSTEM=1000

# --- MCP (Model Context Protocol) Settings ---
MCP_ENABLED=true
# MCP_CONFIG_PATH=/path/to/mcp/config.json
# Timeout for MCP tool calls in milliseconds (increased from 30000 to 120000 for long-running tools)
MCP_DEFAULT_TIMEOUT=120000
MCP_AUTO_CONNECT=true

# --- Agent Settings ---

# --- Agent Event & Artifact Settings ---
AGENT_EMIT_THOUGHTS=true
AGENT_PERSIST_THOUGHTS=true
AGENT_PERSIST_DETAIL_EVENTS=true
AGENT_ARTIFACT_INLINE_MAX_BYTES=4096
AGENT_ARTIFACT_URL_TTL_SECONDS=3600000

# --- Agent Execution Settings ---
# Maximum steps for ReActAgent execution
AGENT_MAX_STEPS=5000
# Maximum output tokens for LLM responses (increased for large file writes)
AGENT_MAX_TOKENS=16384

# --- Agent Session Prewarm (reduce first-request latency) ---
AGENT_SESSION_PREWARM_ENABLED=true
AGENT_SESSION_PREWARM_MAX_PROJECTS=20
AGENT_SESSION_PREWARM_CONCURRENCY=4

# --- Agent Session Pool TTL ---
# Cleanup sessions after inactivity (default: 24 hours)
AGENT_SESSION_TTL_SECONDS=86400

# --- Agent Pool Management (NEW: 3-tier pooled architecture) ---
# Enable the new pooled architecture for Agent instances
# When enabled, provides project-level resource isolation and lifecycle management
AGENT_POOL_ENABLED=false
# Default tier for new projects: hot, warm, cold
# - hot: Dedicated resources, fastest response (for enterprise projects)
# - warm: Shared pool with LRU eviction (default, balanced)
# - cold: On-demand creation with aggressive cleanup (for infrequent use)
AGENT_POOL_DEFAULT_TIER=warm
# Maximum instances in WARM tier shared pool
AGENT_POOL_WARM_MAX_INSTANCES=100
# Maximum instances in COLD tier (on-demand)
AGENT_POOL_COLD_MAX_INSTANCES=200
# COLD tier idle timeout in seconds (default: 5 minutes)
AGENT_POOL_COLD_IDLE_TIMEOUT=300
# Health check interval in seconds
AGENT_POOL_HEALTH_CHECK_INTERVAL=30

# ==========================================
# MemStack Frontend Configuration (Web)
# ==========================================
# These variables must start with VITE_

# API Base URL (For frontend to connect to backend)
VITE_API_URL=http://localhost:8000

# WebSocket URL (Optional, defaults to window.location.host)
# VITE_WS_URL=localhost:8000
