# ==========================================
# MemStack Backend Configuration
# ==========================================

# --- API Server Settings ---
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
# Comma-separated list of allowed origins for CORS
API_ALLOWED_ORIGINS=*

# --- Security Settings ---
# Generate a secure key for production: openssl rand -hex 32
SECRET_KEY=2a0237a3dbd413a52f04f0c4a0e6b183f16694229e8dca3a9b8eb9e33c7d3419
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
REQUIRE_API_KEY=true
API_KEY_HEADER_NAME=Authorization
USE_LITELM=true

# LLM Provider API Key Encryption (32 bytes = 64 hex chars)
# Generate with: python -c "import os; print(os.urandom(32).hex())"
# If not set or invalid, a random development key is used (NOT FOR PRODUCTION)
LLM_ENCRYPTION_KEY=2a0237a3dbd413a52f04f0c4a0e6b183f16694229e8dca3a9b8eb9e33c7d3419

# --- Database Settings ---
# Neo4j Graph Database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password_here

# PostgreSQL for Metadata
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=memstack
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password_here

# PostgreSQL Connection Pool (High Concurrency)
POSTGRES_POOL_SIZE=20
POSTGRES_MAX_OVERFLOW=40
POSTGRES_POOL_RECYCLE=3600
POSTGRES_POOL_PRE_PING=true

# PostgreSQL Read Replica (Optional - for read scaling)
# POSTGRES_READ_REPLICA_HOST=localhost
# POSTGRES_READ_REPLICA_PORT=5434

# Redis for Caching
REDIS_HOST=localhost
REDIS_PORT=6379
# REDIS_PASSWORD=your_redis_password

# --- LLM Provider Settings ---
# Select Provider: 'gemini', 'qwen', 'openai', 'deepseek', 'zai', 'kimi', or 'anthropic'
LLM_PROVIDER=zai

# Google Gemini (Default)
# Get key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.5-flash
GEMINI_EMBEDDING_MODEL=text-embedding-004
GEMINI_RERANK_MODEL=gemini-2.5-flash

# Alibaba Cloud Qwen (通义千问)
# Get key from: https://dashscope.console.aliyun.com/
DASHSCOPE_API_KEY=your_dashscope_api_key_here
QWEN_MODEL=qwen-plus
QWEN_SMALL_MODEL=qwen-turbo
QWEN_EMBEDDING_MODEL=text-embedding-v3
QWEN_RERANK_MODEL=qwen-plus
QWEN_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# OpenAI
# Get key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o
OPENAI_SMALL_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_RERANK_MODEL=gpt-4o-mini

# Deepseek
# Get key from: https://platform.deepseek.com/api_keys
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_SMALL_MODEL=deepseek-coder
DEEPSEEK_RERANK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# ZhipuAI (智普AI)
# Get key from: https://open.bigmodel.cn/usercenter/apikeys
ZHIPU_API_KEY=your_zhipu_api_key_here
ZHIPU_MODEL=glm-4-plus
ZHIPU_SMALL_MODEL=glm-4-flash
ZHIPU_EMBEDDING_MODEL=embedding-3
ZHIPU_RERANK_MODEL=glm-4-flash
ZHIPU_BASE_URL=https://open.bigmodel.cn/api/paas/v4

# Kimi (Moonshot AI)
# Get key from: https://platform.moonshot.cn/console/api-keys
KIMI_API_KEY=your_kimi_api_key_here
KIMI_MODEL=moonshot-v1-8k
KIMI_SMALL_MODEL=moonshot-v1-8k
KIMI_EMBEDDING_MODEL=
KIMI_RERANK_MODEL=moonshot-v1-8k
KIMI_BASE_URL=https://api.moonshot.cn/v1

# Anthropic (Claude)
# Get key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-sonnet-4-20250514
ANTHROPIC_SMALL_MODEL=claude-haiku-4-20250514
ANTHROPIC_EMBEDDING_MODEL=
ANTHROPIC_RERANK_MODEL=claude-haiku-4-20250514
# ANTHROPIC_BASE_URL=  # Optional: custom base URL

# --- Embedding Management ---
# Automatically clear old embeddings when switching LLM providers with different dimensions
# Set to 'false' to disable automatic clearing
AUTO_CLEAR_MISMATCHED_EMBEDDINGS=true

# --- Web Search & Scraping Settings ---
# Tavily API (Web Search)
# Get key from: https://api.tavily.com/
TAVILY_API_KEY=your_tavily_api_key_here
TAVILY_MAX_RESULTS=10
TAVILY_SEARCH_DEPTH=basic
# TAVILY_INCLUDE_DOMAINS=["wikipedia.org", "github.com"]
# TAVILY_EXCLUDE_DOMAINS=["spam.com"]

# Playwright (Web Scraping)
PLAYWRIGHT_TIMEOUT=30000
PLAYWRIGHT_HEADLESS=true
PLAYWRIGHT_MAX_CONTENT_LENGTH=10000

# Cache TTL for web search results (seconds)
WEB_SEARCH_CACHE_TTL=3600

# --- Graphiti Settings ---
GRAPHITI_SEMAPHORE_LIMIT=10
MAX_ASYNC_WORKERS=20
RUN_BACKGROUND_WORKERS=true
QUEUE_BATCH_SIZE=1

# --- LLM Cache & Timeout Settings ---
LLM_TIMEOUT=300
LLM_STREAM_TIMEOUT=600
LLM_CONCURRENCY_LIMIT=8
LLM_MAX_RETRIES=3
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=3600

# --- Monitoring & Telemetry ---
ENABLE_METRICS=true
METRICS_PORT=9090

# OpenTelemetry (Distributed Tracing & Metrics)
ENABLE_TELEMETRY=true
SERVICE_NAME=memstack
ENVIRONMENT=development

# OTLP Exporter (OpenTelemetry Collector)
# Use gRPC endpoint (recommended): http://localhost:4317
# Use HTTP endpoint: http://localhost:4318
# For production with TLS: https://your-collector.example.com:4317
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
# OTEL_EXPORTER_OTLP_HEADERS=authorization=Bearer your-token
# OTEL_EXPORTER_OTLP_PROTOCOL=grpc

# Tracing Sampling (0.0 to 1.0, where 1.0 = 100% sampling)
OTEL_TRACES_SAMPLER=traceidratio
OTEL_TRACES_SAMPLER_ARG=1.0

# Batch export timeout (milliseconds)
OTEL_BATCH_TIMEOUT=30000

# Resource attributes (optional)
# OTEL_RESOURCE_ATTRIBUTES=service.version=1.0.0,deployment.environment=production

# Langfuse LLM Observability (Self-hosted V3)
# Enable/disable Langfuse tracing for all LLM calls
LANGFUSE_ENABLED=false
# Get these from Langfuse UI: http://localhost:3001 -> Settings -> API Keys
# LANGFUSE_PUBLIC_KEY=pk-lf-xxx
# LANGFUSE_SECRET_KEY=sk-lf-xxx
# Self-hosted Langfuse URL (see docker-compose.yml for setup)
LANGFUSE_HOST=http://localhost:3001
# Sampling rate: 1.0 = trace all requests, 0.1 = 10% sampling
LANGFUSE_SAMPLE_RATE=1.0

# Langfuse Docker Compose Settings (for self-hosted deployment)
# Security keys - CHANGE THESE IN PRODUCTION!
# Generate with: openssl rand -hex 32
# LANGFUSE_NEXTAUTH_SECRET=memstack-langfuse-secret-change-in-production
# LANGFUSE_SALT=memstack-langfuse-salt-change-in-production
# LANGFUSE_ENCRYPTION_KEY=0000000000000000000000000000000000000000000000000000000000000000

# --- Logging ---
LOG_LEVEL=INFO
# Options: json, text
LOG_FORMAT=json

# --- S3/MinIO Storage Settings ---
# For AWS S3
# AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=your_aws_access_key_here
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key_here
# S3_BUCKET_NAME=memstack-files

# For local MinIO (development)
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
S3_BUCKET_NAME=memstack-files
S3_ENDPOINT_URL=http://localhost:9000
PRESIGNED_URL_EXPIRATION=3600

# --- Sandbox Code Execution Settings ---
SANDBOX_DEFAULT_PROVIDER=docker
SANDBOX_DEFAULT_IMAGE=sandbox-mcp-server:latest
SANDBOX_TIMEOUT_SECONDS=300
SANDBOX_MEMORY_LIMIT=2G
SANDBOX_CPU_LIMIT=2
SANDBOX_NETWORK_ISOLATED=true

# --- Agent Skill System (L2 Layer) Settings ---
# Threshold for skill prompt injection (0.5 = medium match score)
AGENT_SKILL_MATCH_THRESHOLD=0.5
# Threshold for direct skill execution (0.8 = high confidence match)
AGENT_SKILL_DIRECT_EXECUTE_THRESHOLD=0.8
# Whether to fallback to LLM when skill execution fails
AGENT_SKILL_FALLBACK_ON_ERROR=true
# Timeout for skill direct execution in seconds (increased from 60 to 300 for long-running tasks)
AGENT_SKILL_EXECUTION_TIMEOUT=300

# --- MCP (Model Context Protocol) Settings ---
MCP_ENABLED=true
# MCP_CONFIG_PATH=/path/to/mcp/config.json
# Timeout for MCP tool calls in milliseconds (increased from 30000 to 120000 for long-running tools)
MCP_DEFAULT_TIMEOUT=120000
MCP_AUTO_CONNECT=true

# --- Agent Worker Settings (Independent Temporal Worker) ---
# Agent worker task queue (separate from main worker for independent scaling)
AGENT_TEMPORAL_TASK_QUEUE=memstack-agent-tasks
# Agent worker max concurrent workflows (default: 50)
AGENT_WORKER_CONCURRENCY=50
# Provider refresh interval in seconds (default: 60)
AGENT_PROVIDER_REFRESH_INTERVAL=60

# --- Agent Event & Artifact Settings ---
AGENT_EMIT_THOUGHTS=true
AGENT_PERSIST_THOUGHTS=true
AGENT_PERSIST_DETAIL_EVENTS=true
AGENT_ARTIFACT_INLINE_MAX_BYTES=4096
AGENT_ARTIFACT_URL_TTL_SECONDS=3600000

# --- Agent Execution Settings ---
# Maximum steps for ReActAgent execution
AGENT_MAX_STEPS=5000
# Maximum output tokens for LLM responses (increased for large file writes)
AGENT_MAX_TOKENS=16384

# --- Agent Session Prewarm (reduce first-request latency) ---
AGENT_SESSION_PREWARM_ENABLED=true
AGENT_SESSION_PREWARM_MAX_PROJECTS=20
AGENT_SESSION_PREWARM_CONCURRENCY=4

# ==========================================
# MemStack Frontend Configuration (Web)
# ==========================================
# These variables must start with VITE_

# API Base URL (For frontend to connect to backend)
VITE_API_URL=http://localhost:8000

# WebSocket URL (Optional, defaults to window.location.host)
# VITE_WS_URL=localhost:8000
