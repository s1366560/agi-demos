{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# memstack-agent LLM 模块\n",
    "\n",
    "本 notebook 演示 memstack-agent 的 LLM 抽象层。\n",
    "\n",
    "## 概述\n",
    "\n",
    "LLM 模块提供：\n",
    "- **Message** - 不可变消息类型（system/user/assistant/tool）\n",
    "- **ChatResponse** - 非流式响应\n",
    "- **StreamChunk** - 流式响应块\n",
    "- **ToolCall** - 工具调用\n",
    "- **LLMConfig** - 不可变配置\n",
    "- **LiteLLMAdapter** - 支持 100+ 提供商的适配器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. 导入必要模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/tiejunsun/github/agi-demos/src')\n",
    "\n",
    "from memstack_agent.llm import (\n",
    "    # Types\n",
    "    Message,\n",
    "    MessageRole,\n",
    "    ChatResponse,\n",
    "    StreamChunk,\n",
    "    ToolCall,\n",
    "    Usage,\n",
    "    # Config\n",
    "    LLMConfig,\n",
    "    anthropic_config,\n",
    "    openai_config,\n",
    "    gemini_config,\n",
    "    deepseek_config,\n",
    "    # Protocol\n",
    "    LLMClient,\n",
    "    # Adapter\n",
    "    LiteLLMAdapter,\n",
    "    create_llm_client,\n",
    ")\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. 消息类型 (Message)\n",
    "\n",
    "Message 是不可变的聊天消息，支持 4 种角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建不同角色的消息\n",
    "system_msg = Message.system(\"You are a helpful assistant.\")\n",
    "user_msg = Message.user(\"What is Python?\")\n",
    "assistant_msg = Message.assistant(\"Python is a programming language.\")\n",
    "\n",
    "print(\"System Message:\")\n",
    "print(json.dumps(system_msg.to_dict(), indent=2))\n",
    "print(\"\\nUser Message:\")\n",
    "print(json.dumps(user_msg.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带工具调用的助手消息\n",
    "tool_call = ToolCall(\n",
    "    id=\"call-001\",\n",
    "    name=\"search_web\",\n",
    "    arguments={\"query\": \"Python programming\"}\n",
    ")\n",
    "\n",
    "assistant_with_tool = Message.assistant(tool_calls=[tool_call])\n",
    "print(\"Assistant with Tool Call:\")\n",
    "print(json.dumps(assistant_with_tool.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具结果消息\n",
    "tool_result = Message.tool_result(\n",
    "    content='{\"results\": [\"Python is...\", \"Python features...\"]}',\n",
    "    tool_call_id=\"call-001\",\n",
    "    name=\"search_web\"\n",
    ")\n",
    "\n",
    "print(\"Tool Result Message:\")\n",
    "print(json.dumps(tool_result.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. 响应类型 (ChatResponse & StreamChunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非流式响应\n",
    "response = ChatResponse(\n",
    "    content=\"Python is a high-level programming language.\",\n",
    "    tool_calls=[],\n",
    "    finish_reason=\"stop\",\n",
    "    usage=Usage(prompt_tokens=20, completion_tokens=15, total_tokens=35),\n",
    "    model=\"gpt-4\"\n",
    ")\n",
    "\n",
    "print(\"ChatResponse:\")\n",
    "print(f\"  Content: {response.content}\")\n",
    "print(f\"  Has tool calls: {response.has_tool_calls}\")\n",
    "print(f\"  Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流式响应块\n",
    "chunks = [\n",
    "    StreamChunk(delta=\"Python \"),\n",
    "    StreamChunk(delta=\"is \"),\n",
    "    StreamChunk(delta=\"awesome.\"),\n",
    "    StreamChunk(delta=\"\", finish_reason=\"stop\"),\n",
    "]\n",
    "\n",
    "print(\"Stream Chunks:\")\n",
    "full_text = \"\"\n",
    "for i, chunk in enumerate(chunks):\n",
    "    full_text += chunk.delta\n",
    "    is_final = \"[FINAL]\" if chunk.is_final else \"\"\n",
    "    print(f\"  {i+1}. delta='{chunk.delta}' {is_final}\")\n",
    "\n",
    "print(f\"\\nFull text: '{full_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. 配置 (LLMConfig)\n",
    "\n",
    "LLMConfig 是不可变配置，提供预设工厂函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本配置\n",
    "config = LLMConfig(\n",
    "    model=\"openai/gpt-4\",\n",
    "    api_key=\"sk-your-key\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "print(\"LLMConfig:\")\n",
    "print(f\"  Model: {config.model}\")\n",
    "print(f\"  Temperature: {config.temperature}\")\n",
    "print(f\"  Max tokens: {config.max_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 with_* 方法创建新配置（不可变模式）\n",
    "creative_config = config.with_temperature(0.9)\n",
    "precise_config = config.with_temperature(0.1)\n",
    "\n",
    "print(\"Original temperature:\", config.temperature)\n",
    "print(\"Creative temperature:\", creative_config.temperature)\n",
    "print(\"Precise temperature:\", precise_config.temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预设配置\n",
    "print(\"Preset Configurations:\")\n",
    "print(f\"  Anthropic: {anthropic_config().model}\")\n",
    "print(f\"  OpenAI: {openai_config().model}\")\n",
    "print(f\"  Gemini: {gemini_config().model}\")\n",
    "print(f\"  DeepSeek: {deepseek_config().model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. LLM 协议 (LLMClient)\n",
    "\n",
    "LLMClient 是一个 Protocol 接口，定义了所有 LLM 客户端必须实现的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMClient 协议定义了两个核心方法：\n",
    "# - generate(): 非流式生成\n",
    "# - stream(): 流式生成\n",
    "\n",
    "print(\"LLMClient Protocol Methods:\")\n",
    "print(\"  1. generate(messages, tools=None, **kwargs) -> ChatResponse\")\n",
    "print(\"  2. stream(messages, tools=None, **kwargs) -> AsyncGenerator[StreamChunk]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. LiteLLM 适配器\n",
    "\n",
    "LiteLLMAdapter 实现了 LLMClient 协议，支持 100+ 提供商。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用工厂函数创建客户端\n",
    "client = create_llm_client(\n",
    "    model=\"openai/gpt-4\",\n",
    "    api_key=\"sk-your-key\",  # 实际使用时替换为真实 API key\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"Created client for model: {client._config.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用预设配置创建客户端\n",
    "config = openai_config(model=\"gpt-4-turbo\", api_key=\"sk-your-key\")\n",
    "client = LiteLLMAdapter(config)\n",
    "\n",
    "print(f\"Client model: {client._config.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证适配器实现了协议\n",
    "print(f\"Is LLMClient: {isinstance(client, LLMClient)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. 实际调用示例（需要 API Key）\n",
    "\n",
    "以下代码展示了如何实际调用 LLM，需要设置有效的 API Key。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "\n",
    "# 取消注释以下代码来实际调用 LLM\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "async def demo_generate():\n",
    "    \"\"\"演示非流式生成。\"\"\"\n",
    "    client = create_llm_client(\n",
    "        model=\"openai/gpt-4\",\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        Message.system(\"You are a helpful assistant.\"),\n",
    "        Message.user(\"What is 2 + 2?\"),\n",
    "    ]\n",
    "    \n",
    "    response = await client.generate(messages)\n",
    "    print(f\"Response: {response.content}\")\n",
    "    print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "# await demo_generate()  # 取消注释来运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demo_stream():\n",
    "    \"\"\"演示流式生成。\"\"\"\n",
    "    client = create_llm_client(\n",
    "        model=\"openai/gpt-4\",\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        Message.user(\"Tell me a short joke.\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"Response: \", end=\"\")\n",
    "    async for chunk in client.stream(messages):\n",
    "        print(chunk.delta, end=\"\", flush=True)\n",
    "    print()  # Newline\n",
    "\n",
    "# await demo_stream()  # 取消注释来运行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 8. 工具调用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memstack_agent.tools.protocol import ToolDefinition\n",
    "\n",
    "# 定义工具\n",
    "search_tool = ToolDefinition(\n",
    "    name=\"search_web\",\n",
    "    description=\"Search the web for information\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "async def demo_tool_calling():\n",
    "    \"\"\"演示工具调用。\"\"\"\n",
    "    client = create_llm_client(\n",
    "        model=\"openai/gpt-4\",\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        Message.user(\"What's the weather in Tokyo?\"),\n",
    "    ]\n",
    "    \n",
    "    # 带工具的生成\n",
    "    response = await client.generate(messages, tools=[search_tool])\n",
    "    \n",
    "    if response.has_tool_calls:\n",
    "        for tc in response.tool_calls:\n",
    "            print(f\"Tool: {tc.name}\")\n",
    "            print(f\"Arguments: {tc.arguments}\")\n",
    "\n",
    "# await demo_tool_calling()  # 取消注释来运行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 9. 不可变性验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有类型都是不可变的 (frozen dataclass)\n",
    "msg = Message.user(\"Hello\")\n",
    "\n",
    "try:\n",
    "    msg.content = \"Changed\"\n",
    "except AttributeError as e:\n",
    "    print(f\"Message is immutable: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LLMConfig(model=\"gpt-4\")\n",
    "\n",
    "try:\n",
    "    config.model = \"gpt-5\"\n",
    "except AttributeError as e:\n",
    "    print(f\"Config is immutable: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本 notebook 演示了 memstack-agent 的 LLM 模块：\n",
    "\n",
    "1. **Message** - 不可变消息类型（4 种角色）\n",
    "2. **ChatResponse / StreamChunk** - 响应类型\n",
    "3. **ToolCall / Usage** - 工具调用和用量追踪\n",
    "4. **LLMConfig** - 不可变配置，支持 with_* 方法\n",
    "5. **预设工厂** - anthropic_config, openai_config, gemini_config, deepseek_config\n",
    "6. **LLMClient Protocol** - 统一接口\n",
    "7. **LiteLLMAdapter** - 支持 100+ 提供商的实现\n",
    "8. **create_llm_client** - 工厂函数\n",
    "\n",
    "### 支持的提供商（通过 LiteLLM）\n",
    "\n",
    "- OpenAI (GPT-4, GPT-3.5)\n",
    "- Anthropic (Claude 3)\n",
    "- Google (Gemini)\n",
    "- DeepSeek\n",
    "- Azure OpenAI\n",
    "- AWS Bedrock\n",
    "- Ollama (本地模型)\n",
    "- 以及更多..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
